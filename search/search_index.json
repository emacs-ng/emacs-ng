{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"emacs-ng # A new approach to Emacs homepage \u2022 Deno/Javascript \u2022 webrender \u2022 ng-module \u2022 handbook \u2022 faq Intro # emacs-ng is based off of the master branch of emacs, and regularly merges using Github Actions(Runs at 00:00, only on Saturday). The last merged commit is fad7109e52 (06 30 2024). Motivation # The goal of this fork is to explore new development approaches. To accomplish this, we aim to maintain an inclusive and innovative environment. The project is not about replacing elisp with a more popular language like Javascript. We just want to make emacs more approachable for people who don't like lisp as much as we do. Contributions are welcome from anyone and we are always happy to invite new people to the project. We are open towards interesting ideas to make emacs better. Why Emacs-ng # This project should be considered an additive native layer over emacs, bringing features like Deno's Javascript and Async I/O environment, Mozilla's Webrender, and other features in development. emacs-ng's approach is to utilize multiple new development approaches and tools to bring Emacs to the next level. It is maintained by a team that loves Emacs and everything it stands for - being totally introspectable, with a fully customizable and free development environment. We want Emacs to be a editor 40+ years from now that has the flexibility and design to keep up with progressive technology. Contributing # Emacs combined with the rust ecosystem brings a lot of possibilities. If you have any idea for a new feature, just open an issue before starting work so we can give you some feedback. We try to maintain a list of \"new contributor\" friendly issues tagged with \"good first issue\". You should also take a look at our handbook . Features # Webrender # WebRender is a GPU-based 2D rendering engine written in Rust from Mozilla. Firefox, the research web browser Servo, and other GUI frameworks draw with it. emacs-ng use it as a new experimental graphic backend to leverage GPU hardware. Dynamic modules # Warning This feature is currently unmaintained Allow dynamic modules to access more of Emacs's internals . Dynamic modules can be written to take advantage of these extra functionalities when they are available, while at the same time being fully compatible with vanilla Emacs. Deno/Javascript # Note This feature is currently outdated and been disabled. However there is this to bring it back. One of emacs-ng's primary features is integrating the Deno Runtime , which allows execution of JavaScript and Typescript within Emacs. The details of that feature are listed below, however many users would ask themselves WHY JAVASCRIPT? JavaScript is an extremely dynamic language that allows for a user to inspect and control their scripting environment. The key to note is that bringing in Deno isn't JUST JavaScript - it's an ecosystem of powerful tools and approaches that Emacs just doesn't have currently. TypeScript offers an extremely flexible typing system, that allows to user to have compile time control of their scripting, with the flexibility of types \"getting out of the way\" when not needed. Deno uses Google's v8 JavaScript engine, which features an extremely powerful JIT and world-class garbage collector. Usage of modern Async I/O utilizing Rust's Tokio library. Emacs-ng has WebWorker support, meaning that multiple JavaScript engines can be running in parallel within the editor. The only restriction is that only the 'main' JS Engine can directly call lisp functions. Emacs-ng also has WebAssembly support - compile your C module as WebAsm and distribute it to the world. Don't worry about packaging shared libraries or changing module interfaces, everything can be handled and customized by you the user, at the scripting layer. No need to be dependent on native implementation details. Performance # v8's world-class JIT offers the potential for large performance gains. Async I/O from Deno, WebWorkers, and WebAsm, gives you the tools to make Emacs a smoother and faster experience without having to install additional tools to launch as background processes or worry about shared library versions.","title":"Overview"},{"location":"#emacs-ng","text":"A new approach to Emacs homepage \u2022 Deno/Javascript \u2022 webrender \u2022 ng-module \u2022 handbook \u2022 faq","title":"emacs-ng"},{"location":"#intro","text":"emacs-ng is based off of the master branch of emacs, and regularly merges using Github Actions(Runs at 00:00, only on Saturday). The last merged commit is fad7109e52 (06 30 2024).","title":"Intro"},{"location":"#motivation","text":"The goal of this fork is to explore new development approaches. To accomplish this, we aim to maintain an inclusive and innovative environment. The project is not about replacing elisp with a more popular language like Javascript. We just want to make emacs more approachable for people who don't like lisp as much as we do. Contributions are welcome from anyone and we are always happy to invite new people to the project. We are open towards interesting ideas to make emacs better.","title":"Motivation"},{"location":"#why-emacs-ng","text":"This project should be considered an additive native layer over emacs, bringing features like Deno's Javascript and Async I/O environment, Mozilla's Webrender, and other features in development. emacs-ng's approach is to utilize multiple new development approaches and tools to bring Emacs to the next level. It is maintained by a team that loves Emacs and everything it stands for - being totally introspectable, with a fully customizable and free development environment. We want Emacs to be a editor 40+ years from now that has the flexibility and design to keep up with progressive technology.","title":"Why Emacs-ng"},{"location":"#contributing","text":"Emacs combined with the rust ecosystem brings a lot of possibilities. If you have any idea for a new feature, just open an issue before starting work so we can give you some feedback. We try to maintain a list of \"new contributor\" friendly issues tagged with \"good first issue\". You should also take a look at our handbook .","title":"Contributing"},{"location":"#features","text":"","title":"Features"},{"location":"#webrender","text":"WebRender is a GPU-based 2D rendering engine written in Rust from Mozilla. Firefox, the research web browser Servo, and other GUI frameworks draw with it. emacs-ng use it as a new experimental graphic backend to leverage GPU hardware.","title":"Webrender"},{"location":"#dynamic-modules","text":"Warning This feature is currently unmaintained Allow dynamic modules to access more of Emacs's internals . Dynamic modules can be written to take advantage of these extra functionalities when they are available, while at the same time being fully compatible with vanilla Emacs.","title":"Dynamic modules"},{"location":"#denojavascript","text":"Note This feature is currently outdated and been disabled. However there is this to bring it back. One of emacs-ng's primary features is integrating the Deno Runtime , which allows execution of JavaScript and Typescript within Emacs. The details of that feature are listed below, however many users would ask themselves WHY JAVASCRIPT? JavaScript is an extremely dynamic language that allows for a user to inspect and control their scripting environment. The key to note is that bringing in Deno isn't JUST JavaScript - it's an ecosystem of powerful tools and approaches that Emacs just doesn't have currently. TypeScript offers an extremely flexible typing system, that allows to user to have compile time control of their scripting, with the flexibility of types \"getting out of the way\" when not needed. Deno uses Google's v8 JavaScript engine, which features an extremely powerful JIT and world-class garbage collector. Usage of modern Async I/O utilizing Rust's Tokio library. Emacs-ng has WebWorker support, meaning that multiple JavaScript engines can be running in parallel within the editor. The only restriction is that only the 'main' JS Engine can directly call lisp functions. Emacs-ng also has WebAssembly support - compile your C module as WebAsm and distribute it to the world. Don't worry about packaging shared libraries or changing module interfaces, everything can be handled and customized by you the user, at the scripting layer. No need to be dependent on native implementation details.","title":"Deno/Javascript"},{"location":"#performance","text":"v8's world-class JIT offers the potential for large performance gains. Async I/O from Deno, WebWorkers, and WebAsm, gives you the tools to make Emacs a smoother and faster experience without having to install additional tools to launch as background processes or worry about shared library versions.","title":"Performance"},{"location":"faq/","text":"JavaScript FAQ # Is JavaScript supposed to replace elisp? # The answer here is a loud NO. The maintainers love elisp and we will never remove elisp functionality from emacs-ng. JavaScript/TypeScript are peer languages in the emacs-ng ecosystem, meaning that if new maintainers want to write a package fully in JS/TS, they have that option. They have the full lisp API for interacting with the editor available to them. How should I use JS/TS as an existing package maintainer? # If you have a large elisp package, our guidance is not that you should rewrite your entire package in JS/TS. Instead, we encourage package maintainers to explore using JS/TS's Async I/O and Threading capabilities to improve performance their hot code paths on emacs-ng. Using (featurep 'emacs-ng) , you can include an import statement for a JS/TS package that defun's functions for you to use. Our Getting Started Guide is a good place to start. How does adding JS/TS affect your ability to merge future emacs improvements? # JS/TS is almost a completely additive layer, we have made extremely minimal C changes. As of writing this FAQ, we have only made a small edit to a single line of C to support JS/TS. WebRender, while still in development, has also made minimal C changes. We have the ability to cleanly merge upstream patches without conflict. emacs-ng is based off of the native-comp branch of emacs, and regularly merges in the latest from that branch. emacs-ng can be compiled with nativecomp using ./configure --with-native-compilation . How does JS/TS running affect performance? # JS/TS is a \"you pay for what you use\" system. The JS runtime starts uninitialized, and will not be initialized until you run JavaScript. The JS/TS event loop only runs when you have a pending async operation, including timer callbacks. If you don't have any pending promises/callbacks, the event loop isn't running. This means that impact is proportional to how you use the runtime. If I'm chasing performance, why not just write a C/Rust module? # JS/TS features faster iteration speed and easier distribution for you as the developer. In addition, your users get a greater deal of freedom and customization for your package because it's all in the scripting layer, as opposed to a binary blob they would have to recompile if they wanted to edit the behavior of the code. Instead of building your .so, uploading to a package repository, and dealing with user complaints when that .so isn't loaded properly, you can distribute your script files and still get a considerable performance increase. Will you provide TypeScript definitions for elisp functions? # We do not currently offer that, but it is planned work. We welcome contributions for that effort. emacs-ng seems to be using a large amount of virtual memory? # This is due to Tokio/v8 loading up a potentially large number of worker threads based on your core count. You will notice that real memory committed goes up very little from initializing the runtime. In standard emacs, we have observed emacs sitting at about 700K rss vs 900K rss with the JS runtime initialized. Overall, the real memory overhead of initializing is not as bad as the virtual commitment makes it seem. I have an existing node package I want to use, but import isn't working # We use the Deno Framework for our JavaScript. Deno uses actual ES6 imports, and not node's commonJS require syntax. You will need to use Deno's compatability module to use require syntax. See https://github.com/denoland/deno/tree/master/std/node","title":"FAQ"},{"location":"faq/#javascript-faq","text":"","title":"JavaScript FAQ"},{"location":"faq/#is-javascript-supposed-to-replace-elisp","text":"The answer here is a loud NO. The maintainers love elisp and we will never remove elisp functionality from emacs-ng. JavaScript/TypeScript are peer languages in the emacs-ng ecosystem, meaning that if new maintainers want to write a package fully in JS/TS, they have that option. They have the full lisp API for interacting with the editor available to them.","title":"Is JavaScript supposed to replace elisp?"},{"location":"faq/#how-should-i-use-jsts-as-an-existing-package-maintainer","text":"If you have a large elisp package, our guidance is not that you should rewrite your entire package in JS/TS. Instead, we encourage package maintainers to explore using JS/TS's Async I/O and Threading capabilities to improve performance their hot code paths on emacs-ng. Using (featurep 'emacs-ng) , you can include an import statement for a JS/TS package that defun's functions for you to use. Our Getting Started Guide is a good place to start.","title":"How should I use JS/TS as an existing package maintainer?"},{"location":"faq/#how-does-adding-jsts-affect-your-ability-to-merge-future-emacs-improvements","text":"JS/TS is almost a completely additive layer, we have made extremely minimal C changes. As of writing this FAQ, we have only made a small edit to a single line of C to support JS/TS. WebRender, while still in development, has also made minimal C changes. We have the ability to cleanly merge upstream patches without conflict. emacs-ng is based off of the native-comp branch of emacs, and regularly merges in the latest from that branch. emacs-ng can be compiled with nativecomp using ./configure --with-native-compilation .","title":"How does adding JS/TS affect your ability to merge future emacs improvements?"},{"location":"faq/#how-does-jsts-running-affect-performance","text":"JS/TS is a \"you pay for what you use\" system. The JS runtime starts uninitialized, and will not be initialized until you run JavaScript. The JS/TS event loop only runs when you have a pending async operation, including timer callbacks. If you don't have any pending promises/callbacks, the event loop isn't running. This means that impact is proportional to how you use the runtime.","title":"How does JS/TS running affect performance?"},{"location":"faq/#if-im-chasing-performance-why-not-just-write-a-crust-module","text":"JS/TS features faster iteration speed and easier distribution for you as the developer. In addition, your users get a greater deal of freedom and customization for your package because it's all in the scripting layer, as opposed to a binary blob they would have to recompile if they wanted to edit the behavior of the code. Instead of building your .so, uploading to a package repository, and dealing with user complaints when that .so isn't loaded properly, you can distribute your script files and still get a considerable performance increase.","title":"If I'm chasing performance, why not just write a C/Rust module?"},{"location":"faq/#will-you-provide-typescript-definitions-for-elisp-functions","text":"We do not currently offer that, but it is planned work. We welcome contributions for that effort.","title":"Will you provide TypeScript definitions for elisp functions?"},{"location":"faq/#emacs-ng-seems-to-be-using-a-large-amount-of-virtual-memory","text":"This is due to Tokio/v8 loading up a potentially large number of worker threads based on your core count. You will notice that real memory committed goes up very little from initializing the runtime. In standard emacs, we have observed emacs sitting at about 700K rss vs 900K rss with the JS runtime initialized. Overall, the real memory overhead of initializing is not as bad as the virtual commitment makes it seem.","title":"emacs-ng seems to be using a large amount of virtual memory?"},{"location":"faq/#i-have-an-existing-node-package-i-want-to-use-but-import-isnt-working","text":"We use the Deno Framework for our JavaScript. Deno uses actual ES6 imports, and not node's commonJS require syntax. You will need to use Deno's compatability module to use require syntax. See https://github.com/denoland/deno/tree/master/std/node","title":"I have an existing node package I want to use, but import isn't working"},{"location":"ng-module/","text":"Dynamic Modules # Warning This feature is currently unmaintained Emacs-ng is always built with dynamic modules support enabled, and is fully compatible with dynamic modules written for \"vanilla\" Emacs. On top of the existing emacs-module.h interface, Emacs-ng provides additional extensions that allow dynamic modules to access more of Emacs's internals . Dynamic modules can be written to take advantage of these extra functionalities when they are available, while at the same time being fully compatible with vanilla Emacs. The additional extensions are exposed as a registry of named native functions that can be looked up at run time. These native functions are called ng-module functions : ELISP> ( ng-module-function-address \"ng_module_access_current_buffer_contents\" ) # <user-ptr ptr=0x10e31120d finalizer=0x0> ELISP> ( ng-module-function-address \"non_existing_or_removed_function\" ) nil Unlike normal module functions from emacs_env , these ng-module functions have globally stable addresses . Therefore, the lookup can (and should) be done once, at module load time, inside emacs_module_init . Also note that, even though the lookup function ng-module-function-address is available to Lisp code, it is intended to be used by dynamic modules' native code. (Lisp code cannot meaningfully use the returned address, anyway.) Once an ng-module function is added, its signature will not change. If a similar ng-module function with improved functionalities is added, it will be given a different name. However, a ng-module function can be removed . Direct access to buffer text # To access a buffer's text, a \"vanilla\" dynamic module has to call a buffer-to-string function, like buffer-substring , then call emacs_env->copy_string_contents (resulting in a memcpy ). The temporary Lisp string is typically discarded right away. This is a potential performance bottleneck in hot code paths, like emacs-tree-sitter 's parsing/querying. A dynamic module can instead use the ng-module function ng_module_access_current_buffer_contents to directly read a buffer's text, without copying, or creating a Lisp string. It returns the pointers to (and the sizes of) the 2 contiguous byte segments before and after the buffer's gap. The caller must not write through the returned pointers, and must ensure that the data is read before it is invalidated . Some operations that may invalidate the data are: buffer modifications, garbage collection (which can be triggered by uses of emacs_env ), arena compaction (which can be triggered by malloc when Emacs is built with REL_ALLOC ). Below is an example of how to use this function in a dynamic module written in Rust: use std :: mem :: { self , MaybeUninit }; use once_cell :: sync :: OnceCell ; use emacs :: Env ; type AccessBufferContents = unsafe fn ( * mut * const u8 , * mut isize , * mut * const u8 , * mut isize ); #[allow(non_upper_case_globals)] pub static ng_module_access_current_buffer_contents : OnceCell < AccessBufferContents > = OnceCell :: new (); #[emacs::module] fn init ( env : & Env ) -> Result < () > { let get_addr = env . call ( \"symbol-function\" , [ env . intern ( \"ng-module-function-address\" ) ? ]) ? ; // Got the registry. if get_addr . is_not_nil () { // Look up the ng-module function. match get_addr . call (( \"ng_module_access_current_buffer_contents\" ,)) ? . into_rust :: < Option < Value >> () ? { Some ( addr ) => { // Got the pointer, \"cast\" it to the signature promised by ng-module. buffer :: ng_module_access_current_buffer_contents . set ( unsafe { mem :: transmute ( addr . get_user_ptr () ? ) } ). unwrap (); } None => (), } } Ok (()) } pub unsafe fn current_buffer_contents ( _ : & Env ) -> ( & [ u8 ], & [ u8 ]) { let mut before_gap = MaybeUninit :: uninit (); let mut after_gap = MaybeUninit :: uninit (); let mut before_gap_size : isize = 0 ; let mut after_gap_size : isize = 0 ; let get_slices = ng_module_access_current_buffer_contents . get (). unwrap (); get_slices ( before_gap . as_mut_ptr (), & mut before_gap_size , after_gap . as_mut_ptr (), & mut after_gap_size , ); let before_gap_size = before_gap_size ; let after_gap_size = after_gap_size ; ( if before_gap_size > 0 { std :: slice :: from_raw_parts ( before_gap . assume_init (), before_gap_size as usize , ) } else { & [] }, if after_gap_size > 0 { std :: slice :: from_raw_parts ( after_gap . assume_init (), after_gap_size as usize , ) } else { & [] }, ) } A future version of emacs-module-rs may provide a more convenient wrapper for this function.","title":"Dynamic modules"},{"location":"ng-module/#dynamic-modules","text":"Warning This feature is currently unmaintained Emacs-ng is always built with dynamic modules support enabled, and is fully compatible with dynamic modules written for \"vanilla\" Emacs. On top of the existing emacs-module.h interface, Emacs-ng provides additional extensions that allow dynamic modules to access more of Emacs's internals . Dynamic modules can be written to take advantage of these extra functionalities when they are available, while at the same time being fully compatible with vanilla Emacs. The additional extensions are exposed as a registry of named native functions that can be looked up at run time. These native functions are called ng-module functions : ELISP> ( ng-module-function-address \"ng_module_access_current_buffer_contents\" ) # <user-ptr ptr=0x10e31120d finalizer=0x0> ELISP> ( ng-module-function-address \"non_existing_or_removed_function\" ) nil Unlike normal module functions from emacs_env , these ng-module functions have globally stable addresses . Therefore, the lookup can (and should) be done once, at module load time, inside emacs_module_init . Also note that, even though the lookup function ng-module-function-address is available to Lisp code, it is intended to be used by dynamic modules' native code. (Lisp code cannot meaningfully use the returned address, anyway.) Once an ng-module function is added, its signature will not change. If a similar ng-module function with improved functionalities is added, it will be given a different name. However, a ng-module function can be removed .","title":"Dynamic Modules"},{"location":"ng-module/#direct-access-to-buffer-text","text":"To access a buffer's text, a \"vanilla\" dynamic module has to call a buffer-to-string function, like buffer-substring , then call emacs_env->copy_string_contents (resulting in a memcpy ). The temporary Lisp string is typically discarded right away. This is a potential performance bottleneck in hot code paths, like emacs-tree-sitter 's parsing/querying. A dynamic module can instead use the ng-module function ng_module_access_current_buffer_contents to directly read a buffer's text, without copying, or creating a Lisp string. It returns the pointers to (and the sizes of) the 2 contiguous byte segments before and after the buffer's gap. The caller must not write through the returned pointers, and must ensure that the data is read before it is invalidated . Some operations that may invalidate the data are: buffer modifications, garbage collection (which can be triggered by uses of emacs_env ), arena compaction (which can be triggered by malloc when Emacs is built with REL_ALLOC ). Below is an example of how to use this function in a dynamic module written in Rust: use std :: mem :: { self , MaybeUninit }; use once_cell :: sync :: OnceCell ; use emacs :: Env ; type AccessBufferContents = unsafe fn ( * mut * const u8 , * mut isize , * mut * const u8 , * mut isize ); #[allow(non_upper_case_globals)] pub static ng_module_access_current_buffer_contents : OnceCell < AccessBufferContents > = OnceCell :: new (); #[emacs::module] fn init ( env : & Env ) -> Result < () > { let get_addr = env . call ( \"symbol-function\" , [ env . intern ( \"ng-module-function-address\" ) ? ]) ? ; // Got the registry. if get_addr . is_not_nil () { // Look up the ng-module function. match get_addr . call (( \"ng_module_access_current_buffer_contents\" ,)) ? . into_rust :: < Option < Value >> () ? { Some ( addr ) => { // Got the pointer, \"cast\" it to the signature promised by ng-module. buffer :: ng_module_access_current_buffer_contents . set ( unsafe { mem :: transmute ( addr . get_user_ptr () ? ) } ). unwrap (); } None => (), } } Ok (()) } pub unsafe fn current_buffer_contents ( _ : & Env ) -> ( & [ u8 ], & [ u8 ]) { let mut before_gap = MaybeUninit :: uninit (); let mut after_gap = MaybeUninit :: uninit (); let mut before_gap_size : isize = 0 ; let mut after_gap_size : isize = 0 ; let get_slices = ng_module_access_current_buffer_contents . get (). unwrap (); get_slices ( before_gap . as_mut_ptr (), & mut before_gap_size , after_gap . as_mut_ptr (), & mut after_gap_size , ); let before_gap_size = before_gap_size ; let after_gap_size = after_gap_size ; ( if before_gap_size > 0 { std :: slice :: from_raw_parts ( before_gap . assume_init (), before_gap_size as usize , ) } else { & [] }, if after_gap_size > 0 { std :: slice :: from_raw_parts ( after_gap . assume_init (), after_gap_size as usize , ) } else { & [] }, ) } A future version of emacs-module-rs may provide a more convenient wrapper for this function.","title":"Direct access to buffer text"},{"location":"package-management/","text":"Built-in Emacs Lisp packages # Warning This feature has been removed in favor of upstream change emacs-ng distributed with more built-in Emacs Lisp packages than upstream GNU Emacs. For now, we have straight.el 08b0ecf and use-package a7422fb included. straight.el # We are kind of using straight.el as an alternative of package.el , providing ng-straight-bootstrap-at-startup , ng-bootstrap-straight (as equipment of package-enable-at-startup and package-initialize ). Usage # Emacs NG automatically bootstraps straight.el at startup if ng-straight-bootstrap-at-startup is set to t , the default value is nil You must configure the built-in straight.el in the early init file, as the variable ng-straight-bootstrap-at-startup is read before loading the regular init file. There are some variables you may be interested in (some of them must be set before the bootstrap process, if they might affect how straight.el itself is loaded). You can find the details from this section of the straight.el 's documentation. To be compatible with upstream Emacs, you can place the following in your init-file: (unless (fboundp 'ng-bootstrap-straight) (defvar bootstrap-version) (let ((bootstrap-file (expand-file-name \"straight/repos/straight.el/bootstrap.el\" user-emacs-directory)) (bootstrap-version 5)) (unless (file-exists-p bootstrap-file) (with-current-buffer (url-retrieve-synchronously \"https://raw.githubusercontent.com/raxod502/straight.el/develop/install.el\" 'silent 'inhibit-cookies) (goto-char (point-max)) (eval-print-last-sexp))) (load bootstrap-file nil 'nomessage))) For more detailed guide, please refer straight.el's READMD.md . use-package # There are also discussions/efforts to include use-package into upstream. Until then, we temporally included it with emacs-ng .","title":"Package management"},{"location":"package-management/#built-in-emacs-lisp-packages","text":"Warning This feature has been removed in favor of upstream change emacs-ng distributed with more built-in Emacs Lisp packages than upstream GNU Emacs. For now, we have straight.el 08b0ecf and use-package a7422fb included.","title":"Built-in Emacs Lisp packages"},{"location":"package-management/#straightel","text":"We are kind of using straight.el as an alternative of package.el , providing ng-straight-bootstrap-at-startup , ng-bootstrap-straight (as equipment of package-enable-at-startup and package-initialize ).","title":"straight.el"},{"location":"package-management/#usage","text":"Emacs NG automatically bootstraps straight.el at startup if ng-straight-bootstrap-at-startup is set to t , the default value is nil You must configure the built-in straight.el in the early init file, as the variable ng-straight-bootstrap-at-startup is read before loading the regular init file. There are some variables you may be interested in (some of them must be set before the bootstrap process, if they might affect how straight.el itself is loaded). You can find the details from this section of the straight.el 's documentation. To be compatible with upstream Emacs, you can place the following in your init-file: (unless (fboundp 'ng-bootstrap-straight) (defvar bootstrap-version) (let ((bootstrap-file (expand-file-name \"straight/repos/straight.el/bootstrap.el\" user-emacs-directory)) (bootstrap-version 5)) (unless (file-exists-p bootstrap-file) (with-current-buffer (url-retrieve-synchronously \"https://raw.githubusercontent.com/raxod502/straight.el/develop/install.el\" 'silent 'inhibit-cookies) (goto-char (point-max)) (eval-print-last-sexp))) (load bootstrap-file nil 'nomessage))) For more detailed guide, please refer straight.el's READMD.md .","title":"Usage"},{"location":"package-management/#use-package","text":"There are also discussions/efforts to include use-package into upstream. Until then, we temporally included it with emacs-ng .","title":"use-package"},{"location":"webrender/","text":"WebRender # WebRender rendering is a opt-in feature. Usage # WebRender with PGTK # Emacs WebRender now supports Emacs PGTK build. To use WebRender with PGTK: $ ./configure --with-webrender --with-pgtk WebRender with winit(TAO) # Winit is a cross-platform window creation and event loop management library. TAO is is a fork of winit which replaces Linux's port to Gtk, adding support for webkit2gtk, and a lot of Desktop Environment features like a menu bar, system tray, global shortcuts etc. We are only experimenting with them to build a Emacs window system from scratch. It works to some extent. But more details need to be handled before using it in production environment. You've been warned! Using winit # $ ./configure --with-webrender --with-winit Using TAO # $ ./configure --with-webrender --with-winit=tao OpenGL context creation # We have implemented three means (Surfman/Glutin/Gtk3) to create a OpenGL context creation for WebRender. If the default one does not work for you. You can try with another one. --with-wr-gl=[surfman|glutin|gtk3] Surfman is used by default with winit/TAO, Glutin can be used if Surfman does not work on your device. Gtk3 is used by default with PGTK, and can be used with TAO on UNIX. Troubleshooting # Couldn't find any available vsync extension # If you get \"Couldn't find any available vsync extension\" runtime panic, enabling 3D acceleration will fix it. Random crashes with winit(TAO) # Try building with --enable-winit-pselect $ ./configure --with-webrender --with-winit=tao --enable-winit-pselect Black screen/flickering with winit(TAO) and glutin on Linux # TAO uses gtk under the hood on Linux, so you should build with --with-wr-gl=gtk3 to use gtk's gl context creation and avoid the conflict.","title":"Webrender"},{"location":"webrender/#webrender","text":"WebRender rendering is a opt-in feature.","title":"WebRender"},{"location":"webrender/#usage","text":"","title":"Usage"},{"location":"webrender/#webrender-with-pgtk","text":"Emacs WebRender now supports Emacs PGTK build. To use WebRender with PGTK: $ ./configure --with-webrender --with-pgtk","title":"WebRender with PGTK"},{"location":"webrender/#webrender-with-winittao","text":"Winit is a cross-platform window creation and event loop management library. TAO is is a fork of winit which replaces Linux's port to Gtk, adding support for webkit2gtk, and a lot of Desktop Environment features like a menu bar, system tray, global shortcuts etc. We are only experimenting with them to build a Emacs window system from scratch. It works to some extent. But more details need to be handled before using it in production environment. You've been warned!","title":"WebRender with winit(TAO)"},{"location":"webrender/#using-winit","text":"$ ./configure --with-webrender --with-winit","title":"Using winit"},{"location":"webrender/#using-tao","text":"$ ./configure --with-webrender --with-winit=tao","title":"Using TAO"},{"location":"webrender/#opengl-context-creation","text":"We have implemented three means (Surfman/Glutin/Gtk3) to create a OpenGL context creation for WebRender. If the default one does not work for you. You can try with another one. --with-wr-gl=[surfman|glutin|gtk3] Surfman is used by default with winit/TAO, Glutin can be used if Surfman does not work on your device. Gtk3 is used by default with PGTK, and can be used with TAO on UNIX.","title":"OpenGL context creation"},{"location":"webrender/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"webrender/#couldnt-find-any-available-vsync-extension","text":"If you get \"Couldn't find any available vsync extension\" runtime panic, enabling 3D acceleration will fix it.","title":"Couldn't find any available vsync extension"},{"location":"webrender/#random-crashes-with-winittao","text":"Try building with --enable-winit-pselect $ ./configure --with-webrender --with-winit=tao --enable-winit-pselect","title":"Random crashes with winit(TAO)"},{"location":"webrender/#black-screenflickering-with-winittao-and-glutin-on-linux","text":"TAO uses gtk under the hood on Linux, so you should build with --with-wr-gl=gtk3 to use gtk's gl context creation and avoid the conflict.","title":"Black screen/flickering with winit(TAO) and glutin on Linux"},{"location":"build/building/","text":"These are instructions on how to build emacs-ng. Build requirements # You will need Rust installed . The file rust-toolchain indicates the version that gets installed. This happens automatically, so don't override the toolchain manually. IMPORTANT: Whenever the toolchain updates, you have to reinstall rustfmt manually. Linux # You will need a C compiler and toolchain. On Linux, you can do something like: apt install build-essential automake clang libclang-dev Additional requirements: apt install texinfo libjpeg-dev libtiff-dev \\ libgif-dev libxpm-dev libgtk-3-dev gnutls-dev \\ libncurses5-dev libxml2-dev libxt-dev For native-comp you will also need zlib1g-dev libgccjit-9-dev . MacOS # On MacOS, you will need Xcode. brew install gnutls texinfo autoconf To use the installed version of makeinfo instead of the built-in ( /usr/bin/makeinfo ) one, you'll need to make sure /usr/local/opt/texinfo/bin is before /usr/bin in PATH . Mojave install libxml2 headers with: open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg If you want to use native-comp, you will need to compile with ./configure --with-native-compilation . nativecomp will also require: brew install zlib libgccjit It seems to be more difficult to build native-comp on macOS than on Linux. There are several tutorials that provide instructions on how to successfully compile it (no guarantee that they work): https://gist.github.com/mikroskeem/0a5c909c1880408adf732ceba6d3f9ab https://gist.github.com/AllenDang/f019593e65572a8e0aefc96058a2d23e Compile and install # $ ./autogen.sh $ ./configure --enable-rust-debug $ make -j$(nproc) # proc for number of processors (cores) For a release build, don't pass --enable-rust-debug . The Makefile obeys cargo's RUSTFLAGS variable and additional options can be passed to cargo with CARGO_FLAGS. For example: $ make CARGO_FLAGS = \"-vv\" RUSTFLAGS = \"-Zunstable-options --cfg MARKER_DEBUG\" If you want to install it, just use make install You may need to run sudo make install depending on your system configuration. Now emacs should be available at ./src/emacs . We can launch the application via ./src/emacs . We can navigate to the lisp scratchpad by pressing C-x b and hitting enter.","title":"Build from source"},{"location":"build/building/#build-requirements","text":"You will need Rust installed . The file rust-toolchain indicates the version that gets installed. This happens automatically, so don't override the toolchain manually. IMPORTANT: Whenever the toolchain updates, you have to reinstall rustfmt manually.","title":"Build requirements"},{"location":"build/building/#linux","text":"You will need a C compiler and toolchain. On Linux, you can do something like: apt install build-essential automake clang libclang-dev Additional requirements: apt install texinfo libjpeg-dev libtiff-dev \\ libgif-dev libxpm-dev libgtk-3-dev gnutls-dev \\ libncurses5-dev libxml2-dev libxt-dev For native-comp you will also need zlib1g-dev libgccjit-9-dev .","title":"Linux"},{"location":"build/building/#macos","text":"On MacOS, you will need Xcode. brew install gnutls texinfo autoconf To use the installed version of makeinfo instead of the built-in ( /usr/bin/makeinfo ) one, you'll need to make sure /usr/local/opt/texinfo/bin is before /usr/bin in PATH . Mojave install libxml2 headers with: open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg If you want to use native-comp, you will need to compile with ./configure --with-native-compilation . nativecomp will also require: brew install zlib libgccjit It seems to be more difficult to build native-comp on macOS than on Linux. There are several tutorials that provide instructions on how to successfully compile it (no guarantee that they work): https://gist.github.com/mikroskeem/0a5c909c1880408adf732ceba6d3f9ab https://gist.github.com/AllenDang/f019593e65572a8e0aefc96058a2d23e","title":"MacOS"},{"location":"build/building/#compile-and-install","text":"$ ./autogen.sh $ ./configure --enable-rust-debug $ make -j$(nproc) # proc for number of processors (cores) For a release build, don't pass --enable-rust-debug . The Makefile obeys cargo's RUSTFLAGS variable and additional options can be passed to cargo with CARGO_FLAGS. For example: $ make CARGO_FLAGS = \"-vv\" RUSTFLAGS = \"-Zunstable-options --cfg MARKER_DEBUG\" If you want to install it, just use make install You may need to run sudo make install depending on your system configuration. Now emacs should be available at ./src/emacs . We can launch the application via ./src/emacs . We can navigate to the lisp scratchpad by pressing C-x b and hitting enter.","title":"Compile and install"},{"location":"build/docker/","text":"Build emacs-ng using a Docker container # Warning Build emacs-ng using a Docker container is currently unmaintained To ease the job of building emacs-ng, we are preparing Docker images to allow a fully containerized build without installing any build dependencies. Building this will create a Docker image of about ~9 gb and will take quite some time (~45 minutes, depending on your specs). Once the build is completed, you can extract the artifact out of the container and install it. Usage example of the Dockerfile for building for Debian GNU/Linux: Clone the emacs-ng repository and build the image locally: $ git clone --depth = 1 https://github.com/emacs-ng/emacs-ng.git $ cd emacs-ng $ docker build -t emacs-ng:builder -f docker/Dockerfile.debian . At the end you should read this output: ********************************************************************** Done. The new package has been saved to /src/emacs-ng_0.1-1_amd64.deb You can install it in your system anytime using: dpkg -i emacs-ng_0.1-1_amd64.deb ********************************************************************** Copy the .deb out of the container, then destroy the container: $ docker run -d --rm --name delete-me emacs-ng:builder bash -c 'tail -f /dev/null' --stop-signal SIGKILL $ docker cp delete-me:/src/emacs-ng_0.1-1_amd64.deb ~/tmp/ $ docker stop delete-me","title":"Build with Docker"},{"location":"build/docker/#build-emacs-ng-using-a-docker-container","text":"Warning Build emacs-ng using a Docker container is currently unmaintained To ease the job of building emacs-ng, we are preparing Docker images to allow a fully containerized build without installing any build dependencies. Building this will create a Docker image of about ~9 gb and will take quite some time (~45 minutes, depending on your specs). Once the build is completed, you can extract the artifact out of the container and install it. Usage example of the Dockerfile for building for Debian GNU/Linux: Clone the emacs-ng repository and build the image locally: $ git clone --depth = 1 https://github.com/emacs-ng/emacs-ng.git $ cd emacs-ng $ docker build -t emacs-ng:builder -f docker/Dockerfile.debian . At the end you should read this output: ********************************************************************** Done. The new package has been saved to /src/emacs-ng_0.1-1_amd64.deb You can install it in your system anytime using: dpkg -i emacs-ng_0.1-1_amd64.deb ********************************************************************** Copy the .deb out of the container, then destroy the container: $ docker run -d --rm --name delete-me emacs-ng:builder bash -c 'tail -f /dev/null' --stop-signal SIGKILL $ docker cp delete-me:/src/emacs-ng_0.1-1_amd64.deb ~/tmp/ $ docker stop delete-me","title":"Build emacs-ng using a Docker container"},{"location":"build/nix-develop/","text":"Using Nix Develop Environment # Install nix from shell # sh <(curl -L https://nixos.org/nix/install) --daemon then exec bash -l reload your bash to load the nix-env Enable Flake Feature Of Nix # echo \"experimental-features = nix-command flakes\" | sudo tee -a /etc/nix/nix.conf sudo pkill nix-daemon NOTE: reload bash and check your nix-version first nix --version nix (Nix) 2.4 Nix flake feature already in emacsNg # nix run github:emacs-ng/emacs-ng (launch emacs locally ) nix build github:emacs-ng/emacs-ng (build emacs locally) nix develop github:emacs-ng/emacs-ng (enter develop emacs environment locally) Also, you can run native nix commands (Nix Stable Version) under emacs-ng repo, such as nix-shell nix-build Using cachix to (download binary cache) speed up build # Without installing cachix nix build github:emacs-ng/emacs-ng --option substituters \"https://emacsng.cachix.org\" --option trusted-public-keys \"emacsng.cachix.org-1:i7wOr4YpdRpWWtShI8bT6V7lOTnPeI7Ho6HaZegFWMI=\" -o emacsNg ls -il emacs #or check ./result/bin/emacs nix-build --option substituters \"https://emacsng.cachix.org\" --option trusted-public-keys \"emacsng.cachix.org-1:i7wOr4YpdRpWWtShI8bT6V7lOTnPeI7Ho6HaZegFWMI=\" Installing cachix nix-env -iA cachix -f https://cachix.org/api/v1/install #install cachix exec bash -l cachix use emacsng # make sure you have saw the output like: Configured https://emacsng.cachix.org binary cache in /home/test/.config/nix/nix.conf #then nix-build #or nix build github:emacs-ng/emacs-ng -o emacsNG ls -il emacsNG Clone Emacs Ng By Nix-Shell And Enable Emacs Cachix # nix-env -iA cachix -f https://cachix.org/api/v1/install exec bash -l cachix use emacsng # make sure you have saw the output like: Configured https://emacsng.cachix.org binary cache in /home/test/.config/nix/nix.conf nix-shell -p git --command \"git clone https://github.com/emacs-ng/emacs-ng.git && cd emacs-ng && nix-shell\" Setting Up Rust Development Environment # Change Rust Version # Nightly Version located nix/rust.nix modify the 2021-01-14 to which your want.( rustOverlay-NightlyCheck ) Example: default = pkgs.rust-bin.nightly.\"2021-03-23\"; Stable version example : default = pkgs.rust-bin.stable.\"1.50.0\"; ( rustOverlay-StableCheck ) Beta version example : default = pkgs.rust-bin.beta.\"2021-03-06\"; ( rustOverlay-StableCheck ) Add Package from RustOverly # The RustOverly supported Packages list that you can find here ( packages-list ) located nix/rust.nix rustOverlaySet = mkOption { type = types . listOf types . str ; default = [ \"rustc\" \"cargo\" \"rustfmt-preview\" \"<add other package which you can found in package-list>\" ]; description = \"Which rust tools to pull from the rust overlay https://github.com/oxalica/rust-overlay/blob/master/manifests/profiles.nix\" ; }; Add rustPackages from nixpkgs channel # located nix/rust.nix and the default nix expression like this rustPackages.clippy . In this cause, most of rust toolchain installed from rustOverly rustPackagesSet = mkOption { type = types . listOf types . str ; default = [ \"clippy\" \"<add custom Rust toolchain package>\" ]; description = \"Which rust tools to pull from the nixpkgs channel package set check valid package from https://search.nixos.org/packages?channel=unstable\" ; \"; }; Add nixpkgs package to current Environment # located nix/rust.nix first, search for package in search-package then add the name of package to: devshell . packages = map ( tool : cfg . rustPackages . $ { tool }) cfg . rustPackagesSet ++ map ( tool : cfg . rustOverlay . $ { tool }) cfg . rustOverlaySet ++ ( with pkgs ;[ #custom nixpkgs packages rustracer < add package here > ]); ``` ## Reload all of environments when you changed something - normally , we can re-enter ` nix-shell ` to reload environments . But for this project , we are using direnv to load and unload environment variables in an convenient way . ### Recommended Way -> Direnv Install direnv by Nix nix-env - i direnv and hook direnv to your bash [ direnv-hook ]( https://direnv.net/docs/hook.html ) - using ` direnv allow ` or ` direnv deny ` to enable or disable load ` nix-shell ` in current path - using ` direnv reload ` to reload ` nix-shell ` - Also , you can put as follows to `~ /.config/direnv/direnvrc ` watching the envrs every time changes that could be reload automatically . ``` bash use_flake () { watch_file flake . nix watch_file flake . lock eval \"$(nix print-dev-env)\" } Add Direnv Support With Emacs # https://github.com/purcell/envrc Install direnv to Doom Emacs, Example ( package! envrc :recipe ( :host github :repo \"purcell/envrc\" )) ( use-package! envrc :hook ( after-init . envrc-global-mode ) ) then M-x typing envrv-<**> check related commands similar to native direnv-commands Write A Custom Command Or Environment Variable # locaed nix/commands.toml Add custom command as following: Example [[commands]] name = \"\" command = \"cargo build --manifest-path=./crates/codegen/Cargo.toml\" help = \"cargo build codegen\" category = \"rust-build\" Add custom env variable as following: [[env]] name = \"TEST\" vale = \"/bin/test\" #prefix = \"$( cd \"$(dirname \"$\\{\\BASH_SOURCE [ 0 ]}\")\"; pwd )\" can be prefix Building Emacs-ng in development mode # located flake.nix commented emacsNG-src to \"./.\" Example: let #emacsNgSource = emacsNG-src; emacsNgSource = \"./.\" ; in then nix-build it; Add custom configFlags to emacsNg build process # located flake.nix configureFlags = ( old . configureFlags or [ ]) ++ [ \"--with-json\" \"--with-threads\" \"--with-included-regex\" \"--with-harfbuzz\" \"--with-compress-install\" \"--with-zlib\" \"<custom flags>\" ]; NOTICE: nix-build action in sandbox mode. If you want to modify something or patch it, please put it this way to the corresponding step. For example: preConfigure = ( old . preConfigure or \"\" ) + '' <modify shell> '' ; patches = ( old . patches or [ ]) ++ [ . /nix / <your-pathc-file.patch> ]; if everything looks good, you can run nix-build right now.","title":"Nix Build / Develop"},{"location":"build/nix-develop/#using-nix-develop-environment","text":"","title":"Using Nix Develop Environment"},{"location":"build/nix-develop/#install-nix-from-shell","text":"sh <(curl -L https://nixos.org/nix/install) --daemon then exec bash -l reload your bash to load the nix-env","title":"Install nix from shell"},{"location":"build/nix-develop/#enable-flake-feature-of-nix","text":"echo \"experimental-features = nix-command flakes\" | sudo tee -a /etc/nix/nix.conf sudo pkill nix-daemon NOTE: reload bash and check your nix-version first nix --version nix (Nix) 2.4","title":"Enable Flake Feature Of Nix"},{"location":"build/nix-develop/#nix-flake-feature-already-in-emacsng","text":"nix run github:emacs-ng/emacs-ng (launch emacs locally ) nix build github:emacs-ng/emacs-ng (build emacs locally) nix develop github:emacs-ng/emacs-ng (enter develop emacs environment locally) Also, you can run native nix commands (Nix Stable Version) under emacs-ng repo, such as nix-shell nix-build","title":"Nix flake feature already in emacsNg"},{"location":"build/nix-develop/#using-cachix-to-download-binary-cache-speed-up-build","text":"Without installing cachix nix build github:emacs-ng/emacs-ng --option substituters \"https://emacsng.cachix.org\" --option trusted-public-keys \"emacsng.cachix.org-1:i7wOr4YpdRpWWtShI8bT6V7lOTnPeI7Ho6HaZegFWMI=\" -o emacsNg ls -il emacs #or check ./result/bin/emacs nix-build --option substituters \"https://emacsng.cachix.org\" --option trusted-public-keys \"emacsng.cachix.org-1:i7wOr4YpdRpWWtShI8bT6V7lOTnPeI7Ho6HaZegFWMI=\" Installing cachix nix-env -iA cachix -f https://cachix.org/api/v1/install #install cachix exec bash -l cachix use emacsng # make sure you have saw the output like: Configured https://emacsng.cachix.org binary cache in /home/test/.config/nix/nix.conf #then nix-build #or nix build github:emacs-ng/emacs-ng -o emacsNG ls -il emacsNG","title":"Using cachix to (download binary cache) speed up build"},{"location":"build/nix-develop/#clone-emacs-ng-by-nix-shell-and-enable-emacs-cachix","text":"nix-env -iA cachix -f https://cachix.org/api/v1/install exec bash -l cachix use emacsng # make sure you have saw the output like: Configured https://emacsng.cachix.org binary cache in /home/test/.config/nix/nix.conf nix-shell -p git --command \"git clone https://github.com/emacs-ng/emacs-ng.git && cd emacs-ng && nix-shell\"","title":"Clone Emacs Ng By Nix-Shell And Enable Emacs Cachix"},{"location":"build/nix-develop/#setting-up-rust-development-environment","text":"","title":"Setting Up Rust Development Environment"},{"location":"build/nix-develop/#change-rust-version","text":"Nightly Version located nix/rust.nix modify the 2021-01-14 to which your want.( rustOverlay-NightlyCheck ) Example: default = pkgs.rust-bin.nightly.\"2021-03-23\"; Stable version example : default = pkgs.rust-bin.stable.\"1.50.0\"; ( rustOverlay-StableCheck ) Beta version example : default = pkgs.rust-bin.beta.\"2021-03-06\"; ( rustOverlay-StableCheck )","title":"Change Rust Version"},{"location":"build/nix-develop/#add-package-from-rustoverly","text":"The RustOverly supported Packages list that you can find here ( packages-list ) located nix/rust.nix rustOverlaySet = mkOption { type = types . listOf types . str ; default = [ \"rustc\" \"cargo\" \"rustfmt-preview\" \"<add other package which you can found in package-list>\" ]; description = \"Which rust tools to pull from the rust overlay https://github.com/oxalica/rust-overlay/blob/master/manifests/profiles.nix\" ; };","title":"Add Package from RustOverly"},{"location":"build/nix-develop/#add-rustpackages-from-nixpkgs-channel","text":"located nix/rust.nix and the default nix expression like this rustPackages.clippy . In this cause, most of rust toolchain installed from rustOverly rustPackagesSet = mkOption { type = types . listOf types . str ; default = [ \"clippy\" \"<add custom Rust toolchain package>\" ]; description = \"Which rust tools to pull from the nixpkgs channel package set check valid package from https://search.nixos.org/packages?channel=unstable\" ; \"; };","title":"Add rustPackages from nixpkgs channel"},{"location":"build/nix-develop/#add-nixpkgs-package-to-current-environment","text":"located nix/rust.nix first, search for package in search-package then add the name of package to: devshell . packages = map ( tool : cfg . rustPackages . $ { tool }) cfg . rustPackagesSet ++ map ( tool : cfg . rustOverlay . $ { tool }) cfg . rustOverlaySet ++ ( with pkgs ;[ #custom nixpkgs packages rustracer < add package here > ]); ``` ## Reload all of environments when you changed something - normally , we can re-enter ` nix-shell ` to reload environments . But for this project , we are using direnv to load and unload environment variables in an convenient way . ### Recommended Way -> Direnv Install direnv by Nix nix-env - i direnv and hook direnv to your bash [ direnv-hook ]( https://direnv.net/docs/hook.html ) - using ` direnv allow ` or ` direnv deny ` to enable or disable load ` nix-shell ` in current path - using ` direnv reload ` to reload ` nix-shell ` - Also , you can put as follows to `~ /.config/direnv/direnvrc ` watching the envrs every time changes that could be reload automatically . ``` bash use_flake () { watch_file flake . nix watch_file flake . lock eval \"$(nix print-dev-env)\" }","title":"Add nixpkgs package to current Environment"},{"location":"build/nix-develop/#add-direnv-support-with-emacs","text":"https://github.com/purcell/envrc Install direnv to Doom Emacs, Example ( package! envrc :recipe ( :host github :repo \"purcell/envrc\" )) ( use-package! envrc :hook ( after-init . envrc-global-mode ) ) then M-x typing envrv-<**> check related commands similar to native direnv-commands","title":"Add Direnv Support With Emacs"},{"location":"build/nix-develop/#write-a-custom-command-or-environment-variable","text":"locaed nix/commands.toml Add custom command as following: Example [[commands]] name = \"\" command = \"cargo build --manifest-path=./crates/codegen/Cargo.toml\" help = \"cargo build codegen\" category = \"rust-build\" Add custom env variable as following: [[env]] name = \"TEST\" vale = \"/bin/test\" #prefix = \"$( cd \"$(dirname \"$\\{\\BASH_SOURCE [ 0 ]}\")\"; pwd )\" can be prefix","title":"Write A Custom Command Or Environment Variable"},{"location":"build/nix-develop/#building-emacs-ng-in-development-mode","text":"located flake.nix commented emacsNG-src to \"./.\" Example: let #emacsNgSource = emacsNG-src; emacsNgSource = \"./.\" ; in then nix-build it;","title":"Building Emacs-ng in development mode"},{"location":"build/nix-develop/#add-custom-configflags-to-emacsng-build-process","text":"located flake.nix configureFlags = ( old . configureFlags or [ ]) ++ [ \"--with-json\" \"--with-threads\" \"--with-included-regex\" \"--with-harfbuzz\" \"--with-compress-install\" \"--with-zlib\" \"<custom flags>\" ]; NOTICE: nix-build action in sandbox mode. If you want to modify something or patch it, please put it this way to the corresponding step. For example: preConfigure = ( old . preConfigure or \"\" ) + '' <modify shell> '' ; patches = ( old . patches or [ ]) ++ [ . /nix / <your-pathc-file.patch> ]; if everything looks good, you can run nix-build right now.","title":"Add custom configFlags to emacsNg build process"},{"location":"build/release/","text":"Our CI currently builds binary releases for Ubuntu . Expect more to come: contributions are welcome!","title":"Release .deb package"},{"location":"handbook/build/","text":"emacs-ng build details # Overview # Since emacs-ng is only an additive layer, we have to extend the emacs build system by including the rust code base as a library. The most important emacs files are: configure.ac Makefile.in src/Makefile.in These files contain additions which allow us to dynamically enable the different emacs-ng features. In order to compile the lisp functionality defined in rust, we have to iterate the relevant rust code to find definitions of lisp globals. Cargo build script (build.rs) # This file is the build script of the main crate. In this script we check which features are enabled and create include files that hold bindings for functions and lisp globals. There is a main c_exports.rs file for each crate. This file is included in a crate's lib.rs file. It contains declarations for public rust functions so they can be used from C. Additionally it defines a *_init_syms function that is called by the main init_syms function from the main crate. Unlike the other crates bindings, the related file is under OUT_DIR . Example for main init_syms function with webrender and javascript enabled: #[no_mangle] pub extern \"C\" fn rust_init_syms () { webrender :: webrender_init_syms (); js :: js_init_syms (); ng_module :: ng_module_init_syms (); ng_async :: ng_async_init_syms (); } Generate include files # In the init_syms of a crate, we can find the lisp globals. Files that have include! macro calls at the end have an exports file that is located in the out directory of a crate. Example: include! ( concat! ( env! ( \"CARGO_MANIFEST_DIR\" ), \"/out/javascript_exports.rs\" )); The corresponding include file then contains: export_lisp_fns! { async_handler, call_async_echo, call_async_data_echo, async_send_message, async_close_stream } Here you can see the lisp functions that are defined by the file javascript.rs. Generating rust bindings for C functions with bindgen # The emacs-sys crate is the one calling bindgen which gets called from its build script build.rs . Only C functions are listed in crates/emacs-sys/wrapper.h are considered. We also blacklist several items and define them in rust(for different reasons). The bindings created in each build and can be found in the emacs-sys crate OUT_DIR . There are three files: bindings.rs: functions, structs, enums and more defintions.rs: important types like EmacsInt and USE_LSB_TAG globals.rs: emacs_globals and symbols (e.g. Qnil ) lisp-doc # This crate's purpose is only providing the function scan_rust_file . The function is called by scan_file in make-docfile.c . Besides extracting doc strings from elisp functions, we also use it to find, generate and add the lisp globals we defined in rust. /* Read file FILENAME and output its doc strings to stdout. Return true if file is found, false otherwise. */ static void scan_file ( char * filename ) { ptrdiff_t len = strlen ( filename ); if ( ! generate_globals ) put_filename ( filename ); if ( len > 4 && ! strcmp ( filename + len - 4 , \".elc\" )) scan_lisp_file ( filename , \"rb\" ); else if ( len > 3 && ! strcmp ( filename + len - 3 , \".el\" )) scan_lisp_file ( filename , \"r\" ); else if ( len > 3 && ! strcmp ( filename + len - 3 , \".rs\" )) scan_rust_file ( filename , generate_globals , add_global ); else scan_c_file ( filename , \"r\" ); }","title":"Build"},{"location":"handbook/build/#emacs-ng-build-details","text":"","title":"emacs-ng build details"},{"location":"handbook/build/#overview","text":"Since emacs-ng is only an additive layer, we have to extend the emacs build system by including the rust code base as a library. The most important emacs files are: configure.ac Makefile.in src/Makefile.in These files contain additions which allow us to dynamically enable the different emacs-ng features. In order to compile the lisp functionality defined in rust, we have to iterate the relevant rust code to find definitions of lisp globals.","title":"Overview"},{"location":"handbook/build/#cargo-build-script-buildrs","text":"This file is the build script of the main crate. In this script we check which features are enabled and create include files that hold bindings for functions and lisp globals. There is a main c_exports.rs file for each crate. This file is included in a crate's lib.rs file. It contains declarations for public rust functions so they can be used from C. Additionally it defines a *_init_syms function that is called by the main init_syms function from the main crate. Unlike the other crates bindings, the related file is under OUT_DIR . Example for main init_syms function with webrender and javascript enabled: #[no_mangle] pub extern \"C\" fn rust_init_syms () { webrender :: webrender_init_syms (); js :: js_init_syms (); ng_module :: ng_module_init_syms (); ng_async :: ng_async_init_syms (); }","title":"Cargo build script (build.rs)"},{"location":"handbook/build/#generate-include-files","text":"In the init_syms of a crate, we can find the lisp globals. Files that have include! macro calls at the end have an exports file that is located in the out directory of a crate. Example: include! ( concat! ( env! ( \"CARGO_MANIFEST_DIR\" ), \"/out/javascript_exports.rs\" )); The corresponding include file then contains: export_lisp_fns! { async_handler, call_async_echo, call_async_data_echo, async_send_message, async_close_stream } Here you can see the lisp functions that are defined by the file javascript.rs.","title":"Generate include files"},{"location":"handbook/build/#generating-rust-bindings-for-c-functions-with-bindgen","text":"The emacs-sys crate is the one calling bindgen which gets called from its build script build.rs . Only C functions are listed in crates/emacs-sys/wrapper.h are considered. We also blacklist several items and define them in rust(for different reasons). The bindings created in each build and can be found in the emacs-sys crate OUT_DIR . There are three files: bindings.rs: functions, structs, enums and more defintions.rs: important types like EmacsInt and USE_LSB_TAG globals.rs: emacs_globals and symbols (e.g. Qnil )","title":"Generating rust bindings for C functions with bindgen"},{"location":"handbook/build/#lisp-doc","text":"This crate's purpose is only providing the function scan_rust_file . The function is called by scan_file in make-docfile.c . Besides extracting doc strings from elisp functions, we also use it to find, generate and add the lisp globals we defined in rust. /* Read file FILENAME and output its doc strings to stdout. Return true if file is found, false otherwise. */ static void scan_file ( char * filename ) { ptrdiff_t len = strlen ( filename ); if ( ! generate_globals ) put_filename ( filename ); if ( len > 4 && ! strcmp ( filename + len - 4 , \".elc\" )) scan_lisp_file ( filename , \"rb\" ); else if ( len > 3 && ! strcmp ( filename + len - 3 , \".el\" )) scan_lisp_file ( filename , \"r\" ); else if ( len > 3 && ! strcmp ( filename + len - 3 , \".rs\" )) scan_rust_file ( filename , generate_globals , add_global ); else scan_c_file ( filename , \"r\" ); }","title":"lisp-doc"},{"location":"handbook/getting-started/","text":"Contributor's guide # Getting started # Huge code bases can be intimidating and it always takes some time to get familiar with a project. Don't be afraid to ask questions and to say your opinion. We want to encourage you to share your thoughts with us, so if you have an idea for a new feature or how we can improve the existing ones, feel free to open an issue. You should start by reading our docs. This also allows you to start contributing by improving our docs. Since documentation is very important to get new contibutors on board, this helps us to grow the community. This document is only supposed as an introduction but we will add more doc files that contain detailed information on important parts of the project. Advanced features like webrender or javascript support are documented separately. Overview # Most of the additional functionality of emacs-ng are in rust_src/ . This is the root of the main crate, however the actual code is in rust_src/crates/ , except the crates that are only used for the build. Some crates will only built if you activate the related features when calling ./configure . You can find the features that are used by emacs-ng in Cargo.toml.in . Some of them on by default. Take a look at configure.ac to see how they are activated during the build process. We only apply changes to the emacs code if it's necessary for features that are defined in rust. This way there are less merge conflicts when we perform upstream merges. Bug fixes that also affect emacs can be tracked by an issue, but should but fixed upstream. Build # We use bindgen to generate rust bindings for the functions defined in C. Those bindings are in the crate emacs and are used by other crates through importing them from the emacs crate. The crate codegen holds the code that is responsible for generating the bindings. There's an ongoing effort to explain how emacs-ng is built on top of emacs in handbook/build.md. CI # Since we use github actions, the ci related files are located at .github/workflow . The main job(test.yml) is for building emacs-ng on Linux and osx (including formatting with rustfmt). There's also docs.yml that is responsible for generating and deploying our docs. The remaining files are used to test if the nix build works and to create the release build that runs once a week. Tests # Currently we only run some JavaScript related tests. Because there are several failing native lisp tests, we haven't included them in our build. However there's an open issue that tracks the status. At this point there are no tests that are written in rust. If you want to know how you can run tests, look at handbook/tests.md. The file also contains some test examples. Lisp # The emacs crate also contains most of the code that allows us to make use of elisp types. One of the most important types is LispObject which is the equivalent to the C type Lisp_Object (handbook/types.md). In order to define lisp functions in rust you have to take a look at the macro lisp_fn . Compared to the C version DEFUN it provides a lot more flexibility(handbook/lisp-fn.md). Adding features # The rust ecosystem provides tons of possibilities to improve emacs. We intend to use make use of libgit through git2-rs to improve magit. It would also be cool if we would have a terminal emulator based on rust. If you have any idea for a new feature you want to work on, just add a configure option and create a new crate. Do not hesitate to ask for help. Logs and debugging # If you configure with --enable-rust-debug cargo will build a debug build. We use tracing to collect log output, and a handler (subscriber) is installed in main() if cargo is building a debug build. (The subscriber is not present in release builds, and hence logging is optimised out. Be aware that emitting records can slow down emacs considerably.) The default handler is configured with the EMACSNG_LOG environment variable, in the format target=level . Crate names are targets, so to log for the webrender crate (which is named wrterm ) at the info level, use EMACSNG_LOG=wrterm=info . Multiple filters can be separated with a comma, and filters are capable of advanced filtering with regexes. See the documentation for env_logger for more details.","title":"Getting started"},{"location":"handbook/getting-started/#contributors-guide","text":"","title":"Contributor's guide"},{"location":"handbook/getting-started/#getting-started","text":"Huge code bases can be intimidating and it always takes some time to get familiar with a project. Don't be afraid to ask questions and to say your opinion. We want to encourage you to share your thoughts with us, so if you have an idea for a new feature or how we can improve the existing ones, feel free to open an issue. You should start by reading our docs. This also allows you to start contributing by improving our docs. Since documentation is very important to get new contibutors on board, this helps us to grow the community. This document is only supposed as an introduction but we will add more doc files that contain detailed information on important parts of the project. Advanced features like webrender or javascript support are documented separately.","title":"Getting started"},{"location":"handbook/getting-started/#overview","text":"Most of the additional functionality of emacs-ng are in rust_src/ . This is the root of the main crate, however the actual code is in rust_src/crates/ , except the crates that are only used for the build. Some crates will only built if you activate the related features when calling ./configure . You can find the features that are used by emacs-ng in Cargo.toml.in . Some of them on by default. Take a look at configure.ac to see how they are activated during the build process. We only apply changes to the emacs code if it's necessary for features that are defined in rust. This way there are less merge conflicts when we perform upstream merges. Bug fixes that also affect emacs can be tracked by an issue, but should but fixed upstream.","title":"Overview"},{"location":"handbook/getting-started/#build","text":"We use bindgen to generate rust bindings for the functions defined in C. Those bindings are in the crate emacs and are used by other crates through importing them from the emacs crate. The crate codegen holds the code that is responsible for generating the bindings. There's an ongoing effort to explain how emacs-ng is built on top of emacs in handbook/build.md.","title":"Build"},{"location":"handbook/getting-started/#ci","text":"Since we use github actions, the ci related files are located at .github/workflow . The main job(test.yml) is for building emacs-ng on Linux and osx (including formatting with rustfmt). There's also docs.yml that is responsible for generating and deploying our docs. The remaining files are used to test if the nix build works and to create the release build that runs once a week.","title":"CI"},{"location":"handbook/getting-started/#tests","text":"Currently we only run some JavaScript related tests. Because there are several failing native lisp tests, we haven't included them in our build. However there's an open issue that tracks the status. At this point there are no tests that are written in rust. If you want to know how you can run tests, look at handbook/tests.md. The file also contains some test examples.","title":"Tests"},{"location":"handbook/getting-started/#lisp","text":"The emacs crate also contains most of the code that allows us to make use of elisp types. One of the most important types is LispObject which is the equivalent to the C type Lisp_Object (handbook/types.md). In order to define lisp functions in rust you have to take a look at the macro lisp_fn . Compared to the C version DEFUN it provides a lot more flexibility(handbook/lisp-fn.md).","title":"Lisp"},{"location":"handbook/getting-started/#adding-features","text":"The rust ecosystem provides tons of possibilities to improve emacs. We intend to use make use of libgit through git2-rs to improve magit. It would also be cool if we would have a terminal emulator based on rust. If you have any idea for a new feature you want to work on, just add a configure option and create a new crate. Do not hesitate to ask for help.","title":"Adding features"},{"location":"handbook/getting-started/#logs-and-debugging","text":"If you configure with --enable-rust-debug cargo will build a debug build. We use tracing to collect log output, and a handler (subscriber) is installed in main() if cargo is building a debug build. (The subscriber is not present in release builds, and hence logging is optimised out. Be aware that emitting records can slow down emacs considerably.) The default handler is configured with the EMACSNG_LOG environment variable, in the format target=level . Crate names are targets, so to log for the webrender crate (which is named wrterm ) at the info level, use EMACSNG_LOG=wrterm=info . Multiple filters can be separated with a comma, and filters are capable of advanced filtering with regexes. See the documentation for env_logger for more details.","title":"Logs and debugging"},{"location":"handbook/lisp-fn/","text":"Lisp globals # Functions # This remacs documentation compares the C implementation for the atan function with the ported Rust version. Since emacs-ng isn't about porting the C code base, this example is only intended to show the differences. However these features can be used by functions that are part of new features. The first thing to look at is atan . It takes an optional second argument, which makes it interesting. The complicated mathematical bits, on the other hand, are handled by the standard library. This allows us to focus on the porting process without getting distracted by the math. The Lisp values we are given as arguments are tagged pointers; in this case they are pointers to doubles. The code has to check the tag and follow the pointer to retrieve the real values. Note that this code invokes a C macro (called DEFUN ) that reduces some of the boilerplate. The macro declares a static variable called Satan that holds the metadata the Lisp compiler will need in order to successfully call this function, such as the docstring and the pointer to the Fatan function, which is what the C implementation is named: DEFUN ( \"atan\" , Fatan , Satan , 1 , 2 , 0 , doc : /* Return the inverse tangent of the arguments. If only one argument Y is given, return the inverse tangent of Y. If two arguments Y and X are given, return the inverse tangent of Y divided by X, i.e. the angle in radians between the vector (X, Y) and the x-axis. */ ) ( Lisp_Object y , Lisp_Object x ) { double d = extract_float ( y ); if ( NILP ( x )) d = atan ( d ); else { double d2 = extract_float ( x ); d = atan2 ( d , d2 ); } return make_float ( d ); } extract_float checks the tag (signaling an \"invalid argument\" error if it's not the tag for a double), and returns the actual value. NILP checks to see if the tag indicates that this is a null value, indicating that the user didn't supply a second argument at all. Next take a look at the current Rust implementation. It must also take an optional argument, and it also invokes a (Rust) macro to reduce the boilerplate of declaring the static data for the function. However, it also takes care of all of the type conversions and checks that we need to do in order to handle the arguments and return value: /// Return the inverse tangent of the arguments. /// If only one argument Y is given, return the inverse tangent of Y. /// If two arguments Y and X are given, return the inverse tangent of Y /// divided by X, i.e. the angle in radians between the vector (X, Y) /// and the x-axis #[lisp_fn(min = \"1\" )] pub fn atan ( y : EmacsDouble , x : Option < EmacsDouble > ) -> EmacsDouble { match x { None => y . atan (), Some ( x ) => y . atan2 ( x ) } } You can see that we don't have to check to see if our arguments are of the correct type, the code generated by the lisp_fn macro does this for us. We also asked for the second argument to be an Option<EmacsDouble> . This is the Rust type for a value which is either a valid double or isn't specified at all. We use a match statement to handle both cases. This code is so much better that it's hard to believe just how simple the implementation of the macro is. It just calls .into() on the arguments and the return value; the compiler does the rest when it dispatches this method call to the correct implementation. Attributes # This macro creates the necessary FFI functions and symbols automatically. It handles normal functions and functions that take an arbitrary number of arguments (functions with MANY as the maximum number of arguments on the C side) It is used like this: /// Return the same argument #[lisp_fn(name = \"same\" , c_name = \"same\" , min = \"1\" ] fn same ( obj : LispObject ) -> LispObject { obj } Here the name argument specifies the symbol name that is going to be use in Emacs Lisp, c_name specifies the name for the Fsame and Ssame statics used in C, and min specifies the minimum number of arguments that can be passed to this function, the maximum number of arguments is calculated automatically from the function signature. All three of these arguments are optional, and have sane defaults. Default for name is the Rust function name with _ replaced by - . Default for c_name is the Rust function name. Default for min is the number of Rust arguments, giving a function without optional arguments. In this example the attribute generates the Fsame function that is going to be called in C, and the Ssame structure that holds the function information. You still need to register the function with defsubr to make it visible in Emacs Lisp. To make a function visible to C you need to export it in the crate root (lib.rs) as follows: use somemodule :: Fsome ; Functions with a dynamic number of arguments ( MANY ) # This attribute handles too the definition of functions that take an arbitrary number of arguments, these functions can take an arbitrary number of arguments, but you still can specify a min number of arguments. They are created as follows: /// Returns the first argument. #[lisp_fn(min = \"1\" )] fn first ( args : & mut [ LispObject ]) -> LispObject { args [ 0 ] } Variables # At the end of the C file where the DEFUN is defined there is a called syms_of.... In this file the C code calls defsubr to setup the link between the C code and the Lisp engine. When porting a DEFUN from C, the defsubr call needs to be removed as well. For instance, if syntax-table-p is being ported then find the line like defsubr (&Ssyntax_table_p); and remove it. The all Rust functions declared with lisp_fn have a defsubr line generated for them by the build so there is nothing to do on the Rust side. DEFSYM In C, the DEFSYM macro is used to create an entry in the Lisp symbol table. These are analogous to global variables in the C/Rust code. Like defsubr you will most often see these in the syms_of... functions. When porting DEFUNs check to see if there is a matching DEFSYM as well. If there is remove it from the C and below the ported Rust code add a line like this: def_lisp_sym!(Qsyntax_table_p, \"syntax-table-p\");. Lisp Variables You may also be aware that the C code must quickly and frequently access the current value of a large number of Lisp variables. To make this possible, the C code stores these values in global variables. Yes, lots of global variables. In fact, these aren't just file globals accessible to only one translation unit, these are static variables that are accessible across the whole program. We've started porting these to Rust now as well. DEFVAR_LISP ( \"post-self-insert-hook\" , Vpost_self_insert_hook , doc : /* Hook run at the end of `self-insert-command'. This is run after inserting the character. */ ); Vpost_self_insert_hook = Qnil ; Like DEFUN, DEFVAR_LISP takes both a Lisp name and the C name. The C name becomes the name of the global variable, while the Lisp name is what gets used in Lisp source code. Setting the default value of this variable happens in a separate statement, which is fine. /// Hook run at the end of `self-insert-command'. /// This is run after inserting the character. defvar_lisp ! ( Vpost_self_insert_hook , \"post-self-insert-hook\" , Qnil ); The Rust version must still take both names (this could be simplified if we wrote this macro using a procedural macro), but it also takes a default value. As before, the docstring becomes a comment which all other Rust tooling will recognize. You might be interested in how this is implemented as well: #define DEFVAR_LISP(lname, vname, doc) \\ do { \\ static struct Lisp_Objfwd o_fwd; \\ defvar_lisp (&o_fwd, lname, &globals.f_ ## vname); \\ } while (false) The C macro is not very complicated, but there are two somewhat subtle points. First, it creates an (uninitialized) static variable called o_fwd, of type Lisp_Objfwd. This holds the variable's value, which is a Lisp_Object. It then calls the defvar_lisp function to initialize the fields of this struct, and also to register the variable in the Lisp runtime's global environment, making it accessible to Lisp code. The first subtle point is that every invocation of this marco uses the same variable name, o_fwd. If you call this macro more than once inside the same scope, then they would all be the exact same static variable. Instead the macro body is wrapped inside a do while false loop so that each one has a separate little scope to live in. The other subtlty is that the Lisp_Objfwd struct actually only has a pointer to the value. We still have to allocate some storage for that value somewhere. We take the address of a field on something called globals here. That's the real storage location. This globals object is just a big global struct that holds all the global variables. One day when Emacs is really multi-threaded, there can be one of these per thread and a lot of the rest of the code will just work. #[macro_export] macro_rules! defvar_lisp { ( $field_name : ident , $lisp_name : expr , $value : expr ) => {{ #[allow(unused_unsafe)] unsafe { use $crate :: bindings :: Lisp_Objfwd ; static mut o_fwd : Lisp_Objfwd = Lisp_Objfwd { type_ : $crate :: bindings :: Lisp_Fwd_Type :: Lisp_Fwd_Obj , objvar : unsafe { & $crate :: bindings :: globals . $field_name as * const _ as * mut _ }, }; $crate :: bindings :: defvar_lisp ( & o_fwd , concat! ( $lisp_name , \" \\0 \" ). as_ptr () as * const libc :: c_char , ); $crate :: bindings :: globals . $field_name = $value ; } }}; } The Rust version of this macro is rather longer. Primarily this is because it takes a lot more typing to get a proper uninitialized value in a Rust program. Some would argue that all of this typing is a bad thing, but this is very much an unsafe operation. We're basically promising very precisely that we know this value is uninitialized, and that it will be completely and correctly initialized by the end of this unsafe block. We then call the same defvar_lisp function with the same values, so that the Lisp_Objfwd struct gets initialized and registered in exactly the same way as in the C code. We do have take care to ensure that the Lisp name of the variable is a null-terminated string though.","title":"lisp-fn"},{"location":"handbook/lisp-fn/#lisp-globals","text":"","title":"Lisp globals"},{"location":"handbook/lisp-fn/#functions","text":"This remacs documentation compares the C implementation for the atan function with the ported Rust version. Since emacs-ng isn't about porting the C code base, this example is only intended to show the differences. However these features can be used by functions that are part of new features. The first thing to look at is atan . It takes an optional second argument, which makes it interesting. The complicated mathematical bits, on the other hand, are handled by the standard library. This allows us to focus on the porting process without getting distracted by the math. The Lisp values we are given as arguments are tagged pointers; in this case they are pointers to doubles. The code has to check the tag and follow the pointer to retrieve the real values. Note that this code invokes a C macro (called DEFUN ) that reduces some of the boilerplate. The macro declares a static variable called Satan that holds the metadata the Lisp compiler will need in order to successfully call this function, such as the docstring and the pointer to the Fatan function, which is what the C implementation is named: DEFUN ( \"atan\" , Fatan , Satan , 1 , 2 , 0 , doc : /* Return the inverse tangent of the arguments. If only one argument Y is given, return the inverse tangent of Y. If two arguments Y and X are given, return the inverse tangent of Y divided by X, i.e. the angle in radians between the vector (X, Y) and the x-axis. */ ) ( Lisp_Object y , Lisp_Object x ) { double d = extract_float ( y ); if ( NILP ( x )) d = atan ( d ); else { double d2 = extract_float ( x ); d = atan2 ( d , d2 ); } return make_float ( d ); } extract_float checks the tag (signaling an \"invalid argument\" error if it's not the tag for a double), and returns the actual value. NILP checks to see if the tag indicates that this is a null value, indicating that the user didn't supply a second argument at all. Next take a look at the current Rust implementation. It must also take an optional argument, and it also invokes a (Rust) macro to reduce the boilerplate of declaring the static data for the function. However, it also takes care of all of the type conversions and checks that we need to do in order to handle the arguments and return value: /// Return the inverse tangent of the arguments. /// If only one argument Y is given, return the inverse tangent of Y. /// If two arguments Y and X are given, return the inverse tangent of Y /// divided by X, i.e. the angle in radians between the vector (X, Y) /// and the x-axis #[lisp_fn(min = \"1\" )] pub fn atan ( y : EmacsDouble , x : Option < EmacsDouble > ) -> EmacsDouble { match x { None => y . atan (), Some ( x ) => y . atan2 ( x ) } } You can see that we don't have to check to see if our arguments are of the correct type, the code generated by the lisp_fn macro does this for us. We also asked for the second argument to be an Option<EmacsDouble> . This is the Rust type for a value which is either a valid double or isn't specified at all. We use a match statement to handle both cases. This code is so much better that it's hard to believe just how simple the implementation of the macro is. It just calls .into() on the arguments and the return value; the compiler does the rest when it dispatches this method call to the correct implementation.","title":"Functions"},{"location":"handbook/lisp-fn/#attributes","text":"This macro creates the necessary FFI functions and symbols automatically. It handles normal functions and functions that take an arbitrary number of arguments (functions with MANY as the maximum number of arguments on the C side) It is used like this: /// Return the same argument #[lisp_fn(name = \"same\" , c_name = \"same\" , min = \"1\" ] fn same ( obj : LispObject ) -> LispObject { obj } Here the name argument specifies the symbol name that is going to be use in Emacs Lisp, c_name specifies the name for the Fsame and Ssame statics used in C, and min specifies the minimum number of arguments that can be passed to this function, the maximum number of arguments is calculated automatically from the function signature. All three of these arguments are optional, and have sane defaults. Default for name is the Rust function name with _ replaced by - . Default for c_name is the Rust function name. Default for min is the number of Rust arguments, giving a function without optional arguments. In this example the attribute generates the Fsame function that is going to be called in C, and the Ssame structure that holds the function information. You still need to register the function with defsubr to make it visible in Emacs Lisp. To make a function visible to C you need to export it in the crate root (lib.rs) as follows: use somemodule :: Fsome ;","title":"Attributes"},{"location":"handbook/lisp-fn/#functions-with-a-dynamic-number-of-arguments-many","text":"This attribute handles too the definition of functions that take an arbitrary number of arguments, these functions can take an arbitrary number of arguments, but you still can specify a min number of arguments. They are created as follows: /// Returns the first argument. #[lisp_fn(min = \"1\" )] fn first ( args : & mut [ LispObject ]) -> LispObject { args [ 0 ] }","title":"Functions with a dynamic number of arguments (MANY)"},{"location":"handbook/lisp-fn/#variables","text":"At the end of the C file where the DEFUN is defined there is a called syms_of.... In this file the C code calls defsubr to setup the link between the C code and the Lisp engine. When porting a DEFUN from C, the defsubr call needs to be removed as well. For instance, if syntax-table-p is being ported then find the line like defsubr (&Ssyntax_table_p); and remove it. The all Rust functions declared with lisp_fn have a defsubr line generated for them by the build so there is nothing to do on the Rust side. DEFSYM In C, the DEFSYM macro is used to create an entry in the Lisp symbol table. These are analogous to global variables in the C/Rust code. Like defsubr you will most often see these in the syms_of... functions. When porting DEFUNs check to see if there is a matching DEFSYM as well. If there is remove it from the C and below the ported Rust code add a line like this: def_lisp_sym!(Qsyntax_table_p, \"syntax-table-p\");. Lisp Variables You may also be aware that the C code must quickly and frequently access the current value of a large number of Lisp variables. To make this possible, the C code stores these values in global variables. Yes, lots of global variables. In fact, these aren't just file globals accessible to only one translation unit, these are static variables that are accessible across the whole program. We've started porting these to Rust now as well. DEFVAR_LISP ( \"post-self-insert-hook\" , Vpost_self_insert_hook , doc : /* Hook run at the end of `self-insert-command'. This is run after inserting the character. */ ); Vpost_self_insert_hook = Qnil ; Like DEFUN, DEFVAR_LISP takes both a Lisp name and the C name. The C name becomes the name of the global variable, while the Lisp name is what gets used in Lisp source code. Setting the default value of this variable happens in a separate statement, which is fine. /// Hook run at the end of `self-insert-command'. /// This is run after inserting the character. defvar_lisp ! ( Vpost_self_insert_hook , \"post-self-insert-hook\" , Qnil ); The Rust version must still take both names (this could be simplified if we wrote this macro using a procedural macro), but it also takes a default value. As before, the docstring becomes a comment which all other Rust tooling will recognize. You might be interested in how this is implemented as well: #define DEFVAR_LISP(lname, vname, doc) \\ do { \\ static struct Lisp_Objfwd o_fwd; \\ defvar_lisp (&o_fwd, lname, &globals.f_ ## vname); \\ } while (false) The C macro is not very complicated, but there are two somewhat subtle points. First, it creates an (uninitialized) static variable called o_fwd, of type Lisp_Objfwd. This holds the variable's value, which is a Lisp_Object. It then calls the defvar_lisp function to initialize the fields of this struct, and also to register the variable in the Lisp runtime's global environment, making it accessible to Lisp code. The first subtle point is that every invocation of this marco uses the same variable name, o_fwd. If you call this macro more than once inside the same scope, then they would all be the exact same static variable. Instead the macro body is wrapped inside a do while false loop so that each one has a separate little scope to live in. The other subtlty is that the Lisp_Objfwd struct actually only has a pointer to the value. We still have to allocate some storage for that value somewhere. We take the address of a field on something called globals here. That's the real storage location. This globals object is just a big global struct that holds all the global variables. One day when Emacs is really multi-threaded, there can be one of these per thread and a lot of the rest of the code will just work. #[macro_export] macro_rules! defvar_lisp { ( $field_name : ident , $lisp_name : expr , $value : expr ) => {{ #[allow(unused_unsafe)] unsafe { use $crate :: bindings :: Lisp_Objfwd ; static mut o_fwd : Lisp_Objfwd = Lisp_Objfwd { type_ : $crate :: bindings :: Lisp_Fwd_Type :: Lisp_Fwd_Obj , objvar : unsafe { & $crate :: bindings :: globals . $field_name as * const _ as * mut _ }, }; $crate :: bindings :: defvar_lisp ( & o_fwd , concat! ( $lisp_name , \" \\0 \" ). as_ptr () as * const libc :: c_char , ); $crate :: bindings :: globals . $field_name = $value ; } }}; } The Rust version of this macro is rather longer. Primarily this is because it takes a lot more typing to get a proper uninitialized value in a Rust program. Some would argue that all of this typing is a bad thing, but this is very much an unsafe operation. We're basically promising very precisely that we know this value is uninitialized, and that it will be completely and correctly initialized by the end of this unsafe block. We then call the same defvar_lisp function with the same values, so that the Lisp_Objfwd struct gets initialized and registered in exactly the same way as in the C code. We do have take care to ensure that the Lisp name of the variable is a null-terminated string though.","title":"Variables"},{"location":"handbook/tests/","text":"Tests # Running tests # Run elisp and Rust tests in top level directory. If run in a subdirectory, only run the tests in that directory. make check Run all tests as defined in the directory . Expensive tests are suppressed . The result of the tests for . el is stored in . log . make check - maybe Like \" make check \" , but run only the tests for files that have been modified since the last build . Writing tests # Elisp # For elisp testing, remacs uses ert. Add new tests to test/rust_src/src/ -tests.el. There are good examples in the directory to follow. In general, there should be at least one test function for each Rust function. This function should be a 'smoke' test. Does the Rust call succeed for common values? Does it fail for common values? More complex tests or tests that involve several lisp functions should be defined in a function named after what the test is testing. As an example here is how the if function is tested: ( ert-deftest eval-tests--if-base () \"Check (if) base cases\" ( should-error ( eval ' ( if )) :type 'wrong-number-of-arguments ) ( should ( eq ( if t 'a ) 'a )) ( should ( eq ( if t 'a 'b ) 'a )) ( should ( eq ( if nil 'a ) nil )) ( should ( eq ( if nil 'a 'b ) 'b )) ( should ( eq ( if t 'a ( error \"Not evaluated!\" )) 'a )) ( should ( eq ( if nil ( error \"Not evaluated!\" ) 'a ) 'a ))) ( ert-deftest eval-tests--if-dot-string () \"Check that Emacs rejects (if . \\\"string\\\").\" ( should-error ( eval ' ( if . \"abc\" )) :type 'wrong-type-argument ) ( let (( if-tail ( list ' ( setcdr if-tail \"abc\" ) t ))) ( should-error ( eval ( cons 'if if-tail )))) ( let (( if-tail ( list ' ( progn ( setcdr if-tail \"abc\" ) nil ) t ))) ( should-error ( eval ( cons 'if if-tail ))))) Rust # #[cfg(test)] use std :: cmp :: max ; #[test] fn test_lisp_float_size () { let double_size = mem :: size_of :: < EmacsDouble > (); let ptr_size = mem :: size_of :: <* const Lisp_Float > (); assert! ( mem :: size_of :: < Lisp_Float > () == max ( double_size , ptr_size )); } Running code formatters # Rust # Run cargo fmt in the rust_src/ folder and format all crates in the workspace. make rustfmt","title":"Tests"},{"location":"handbook/tests/#tests","text":"","title":"Tests"},{"location":"handbook/tests/#running-tests","text":"Run elisp and Rust tests in top level directory. If run in a subdirectory, only run the tests in that directory. make check Run all tests as defined in the directory . Expensive tests are suppressed . The result of the tests for . el is stored in . log . make check - maybe Like \" make check \" , but run only the tests for files that have been modified since the last build .","title":"Running tests"},{"location":"handbook/tests/#writing-tests","text":"","title":"Writing tests"},{"location":"handbook/tests/#elisp","text":"For elisp testing, remacs uses ert. Add new tests to test/rust_src/src/ -tests.el. There are good examples in the directory to follow. In general, there should be at least one test function for each Rust function. This function should be a 'smoke' test. Does the Rust call succeed for common values? Does it fail for common values? More complex tests or tests that involve several lisp functions should be defined in a function named after what the test is testing. As an example here is how the if function is tested: ( ert-deftest eval-tests--if-base () \"Check (if) base cases\" ( should-error ( eval ' ( if )) :type 'wrong-number-of-arguments ) ( should ( eq ( if t 'a ) 'a )) ( should ( eq ( if t 'a 'b ) 'a )) ( should ( eq ( if nil 'a ) nil )) ( should ( eq ( if nil 'a 'b ) 'b )) ( should ( eq ( if t 'a ( error \"Not evaluated!\" )) 'a )) ( should ( eq ( if nil ( error \"Not evaluated!\" ) 'a ) 'a ))) ( ert-deftest eval-tests--if-dot-string () \"Check that Emacs rejects (if . \\\"string\\\").\" ( should-error ( eval ' ( if . \"abc\" )) :type 'wrong-type-argument ) ( let (( if-tail ( list ' ( setcdr if-tail \"abc\" ) t ))) ( should-error ( eval ( cons 'if if-tail )))) ( let (( if-tail ( list ' ( progn ( setcdr if-tail \"abc\" ) nil ) t ))) ( should-error ( eval ( cons 'if if-tail )))))","title":"Elisp"},{"location":"handbook/tests/#rust","text":"#[cfg(test)] use std :: cmp :: max ; #[test] fn test_lisp_float_size () { let double_size = mem :: size_of :: < EmacsDouble > (); let ptr_size = mem :: size_of :: <* const Lisp_Float > (); assert! ( mem :: size_of :: < Lisp_Float > () == max ( double_size , ptr_size )); }","title":"Rust"},{"location":"handbook/tests/#running-code-formatters","text":"","title":"Running code formatters"},{"location":"handbook/tests/#rust_1","text":"Run cargo fmt in the rust_src/ folder and format all crates in the workspace. make rustfmt","title":"Rust"},{"location":"handbook/types/","text":"Types # LispObject and Lisp_Object # Lisp_Object (LispObject in Rust) is an integer aka EMACS_INT (EmacsInt in Rust). It is also used as a pointer and bitmasking is used to add extra data to the Lisp_Object. Numbers EmacsInt and EMACS_INT # There are two types of Integers in Emacs Lisp -- Fixnums and Natnums. A fixnum is equivalent to a i64/i32 in Rust and a natnum is u64/u32. In general, use a fixnum unless there is a reason to use a natnum. A fixnum is represented by a EmacsInt and natnum by a EmacsUint. char and i8/u8 # Emacs uses the C char type to represent single byte values. Since char is unsigned by default on ARM platforms and signed on x86 and others, it's important to use the libc::c_char when writing functions that interoperate with the char type. ENUM_BF and bool_bf # Because part of emacs-ng is written in Rust, while the bulk of it is still in C, both the Rust and the C code must be able to call functions written in the other language. Rust makes this fairly painless. It has excellent support for C FFI. However, manually translating function and structure declarations into Rust can be quite painful. Worse, any tiny mistake will come back to haunt you later. Crashes and weird bugs that don't make sense are a very real problem. We had several itermittent bugs that were introduced when a complicated struct was incorrectly translated into Rust, so that parts of the code were stepping on each other. We've fixed this problem by using Bindgen to generate these bindings for us. Aside from saving us a lot of time, Bindgen also gives us relatively nice ways to handle C enums, unions, bitfields, and variable-length structures. Emacs frequently uses these, so this is a great help. First allow me to show you a fairly important C structure called Lisp_Symbol. This struct holds all of the information that Emacs knows about a Lisp symbol. It's got a number of bit fields as well as an internal union. Note that I've elided the comments from this declaration: struct Lisp_Symbol { bool_bf gcmarkbit : 1 ; ENUM_BF ( symbol_redirect ) redirect : 3 ; ENUM_BF ( symbol_trapped_write ) trapped_write : 2 ; unsigned interned : 2 ; bool_bf declared_special : 1 ; bool_bf pinned : 1 ; Lisp_Object name ; union { Lisp_Object value ; struct Lisp_Symbol * alias ; struct Lisp_Buffer_Local_Value * blv ; union Lisp_Fwd * fwd ; } val ; Lisp_Object function ; Lisp_Object plist ; struct Lisp_Symbol * next ; }; ENUM_BF and bool_bf are C preprocessor hacks that allow the code to be compiled even when the compiler doesn't support enums or bools as bitfield types. Bindgen generates the following Rust struct: #[repr(C)] pub struct Lisp_Symbol { pub _bitfield_1 : __BindgenBitfieldUnit < [ u8 ; 2 usize ], u8 > , pub name : Lisp_Object , pub val : Lisp_Symbol__bindgen_ty_1 , pub function : Lisp_Object , pub plist : Lisp_Object , pub next : * mut Lisp_Symbol , } #[repr(C)] pub union Lisp_Symbol__bindgen_ty_1 { pub value : Lisp_Object , pub alias : * mut Lisp_Symbol , pub blv : * mut Lisp_Buffer_Local_Value , pub fwd : * mut Lisp_Fwd , _bindgen_union_align : u64 , } As you can see, the bitfields become rather opaque, they're no longer listed in the struct (you can, however, still see that they occupy two bytes in the struct). Instead, Bindgen creates getter and setter methods and adds them to the impl Lisp_Symbol. I'll just show an excerpt here: impl Lisp_Symbol { #[inline] pub fn gcmarkbit ( & self ) -> bool_bf { unsafe { :: std :: mem :: transmute ( self . _bitfield_1 . get ( 0 usize , 1 u8 ) as u8 ) } } #[inline] pub fn set_gcmarkbit ( & mut self , val : bool_bf ) { unsafe { let val : u8 = :: std :: mem :: transmute ( val ); self . _bitfield_1 . set ( 0 usize , 1 u8 , val as u64 ) } } // ---8<--- } The union is also a little more verbose than before, as it cannot be put anonymously into the rest of the struct. Rust requires that it have a proper name, and so Bindgen has generated one. It's not great, but it'll suffice.","title":"Types"},{"location":"handbook/types/#types","text":"","title":"Types"},{"location":"handbook/types/#lispobject-and-lisp_object","text":"Lisp_Object (LispObject in Rust) is an integer aka EMACS_INT (EmacsInt in Rust). It is also used as a pointer and bitmasking is used to add extra data to the Lisp_Object. Numbers","title":"LispObject and Lisp_Object"},{"location":"handbook/types/#emacsint-and-emacs_int","text":"There are two types of Integers in Emacs Lisp -- Fixnums and Natnums. A fixnum is equivalent to a i64/i32 in Rust and a natnum is u64/u32. In general, use a fixnum unless there is a reason to use a natnum. A fixnum is represented by a EmacsInt and natnum by a EmacsUint.","title":"EmacsInt and EMACS_INT"},{"location":"handbook/types/#char-and-i8u8","text":"Emacs uses the C char type to represent single byte values. Since char is unsigned by default on ARM platforms and signed on x86 and others, it's important to use the libc::c_char when writing functions that interoperate with the char type.","title":"char and i8/u8"},{"location":"handbook/types/#enum_bf-and-bool_bf","text":"Because part of emacs-ng is written in Rust, while the bulk of it is still in C, both the Rust and the C code must be able to call functions written in the other language. Rust makes this fairly painless. It has excellent support for C FFI. However, manually translating function and structure declarations into Rust can be quite painful. Worse, any tiny mistake will come back to haunt you later. Crashes and weird bugs that don't make sense are a very real problem. We had several itermittent bugs that were introduced when a complicated struct was incorrectly translated into Rust, so that parts of the code were stepping on each other. We've fixed this problem by using Bindgen to generate these bindings for us. Aside from saving us a lot of time, Bindgen also gives us relatively nice ways to handle C enums, unions, bitfields, and variable-length structures. Emacs frequently uses these, so this is a great help. First allow me to show you a fairly important C structure called Lisp_Symbol. This struct holds all of the information that Emacs knows about a Lisp symbol. It's got a number of bit fields as well as an internal union. Note that I've elided the comments from this declaration: struct Lisp_Symbol { bool_bf gcmarkbit : 1 ; ENUM_BF ( symbol_redirect ) redirect : 3 ; ENUM_BF ( symbol_trapped_write ) trapped_write : 2 ; unsigned interned : 2 ; bool_bf declared_special : 1 ; bool_bf pinned : 1 ; Lisp_Object name ; union { Lisp_Object value ; struct Lisp_Symbol * alias ; struct Lisp_Buffer_Local_Value * blv ; union Lisp_Fwd * fwd ; } val ; Lisp_Object function ; Lisp_Object plist ; struct Lisp_Symbol * next ; }; ENUM_BF and bool_bf are C preprocessor hacks that allow the code to be compiled even when the compiler doesn't support enums or bools as bitfield types. Bindgen generates the following Rust struct: #[repr(C)] pub struct Lisp_Symbol { pub _bitfield_1 : __BindgenBitfieldUnit < [ u8 ; 2 usize ], u8 > , pub name : Lisp_Object , pub val : Lisp_Symbol__bindgen_ty_1 , pub function : Lisp_Object , pub plist : Lisp_Object , pub next : * mut Lisp_Symbol , } #[repr(C)] pub union Lisp_Symbol__bindgen_ty_1 { pub value : Lisp_Object , pub alias : * mut Lisp_Symbol , pub blv : * mut Lisp_Buffer_Local_Value , pub fwd : * mut Lisp_Fwd , _bindgen_union_align : u64 , } As you can see, the bitfields become rather opaque, they're no longer listed in the struct (you can, however, still see that they occupy two bytes in the struct). Instead, Bindgen creates getter and setter methods and adds them to the impl Lisp_Symbol. I'll just show an excerpt here: impl Lisp_Symbol { #[inline] pub fn gcmarkbit ( & self ) -> bool_bf { unsafe { :: std :: mem :: transmute ( self . _bitfield_1 . get ( 0 usize , 1 u8 ) as u8 ) } } #[inline] pub fn set_gcmarkbit ( & mut self , val : bool_bf ) { unsafe { let val : u8 = :: std :: mem :: transmute ( val ); self . _bitfield_1 . set ( 0 usize , 1 u8 , val as u64 ) } } // ---8<--- } The union is also a little more verbose than before, as it cannot be put anonymously into the rest of the struct. Rust requires that it have a proper name, and so Bindgen has generated one. It's not great, but it'll suffice.","title":"ENUM_BF and bool_bf"},{"location":"js/adv-features/","text":"Advanced Features # This section assumes you have completed \"Getting Started\" and \"Using Deno\" and have a basic familiarization with the elisp JavaScript API. WebWorkers # One of the big draws of emacs-ng is parallel scripting. You may or may not be familiar with the fact that vanilla emacs had lisp threads that you could create via (make-thread FUNC) . Lisp threads were concurrent, but not parallel. In addition to that limitation, lisp threads that were not the 'main thread' would only execute at specific times during emacs event loop. WebWorkers give you parallelism. Under the hood, they are spinning up a Rust std::thread (in C terms, this is analogous to a pthread). Due to this, there is a limited interface to exchange data between threads. Building up the WebWorker # We will start by creating the JavaScript our WebWorker will execute. Create a file named \"web-worker.js\" and insert the following: self . onmessage = ( input ) => { let { message } = input . data ; message += \" And My Axe \" ; self . postMessage ({ message }); self . close (); }; WebWorkers communicate via two special functions, postMessage and onmessage. onmessage will be called once our parent thread sends us a message, while we can use postMessage to send data back to the main thread. This example is appending a string to its input and handing that back to the main thread. We also called the close function, meaning that this WebWorker is \"one and done\", and will shut down once it has performed this operation a single time. If we did not call close, the worker would stay alive and await additional messages. Using our WebWorker # Create a new file with the following code: declare var lisp : any ; const worker = new Worker ( new URL ( \"web-worker.js\" , import . meta . url ). href , { type : \"module\" , deno : true , }); worker . onmessage = ( output ) => { const { message } = output . data ; // This is safe because our callback is back // on the main thread. lisp . print ( message ); } worker . postMessage ({ message : \"You have my sword .... \" }); This code spins up our WebWorker and passes it a message. Running this code with eval-ts-buffer should print a reference to a cool little obscure movie in your minibuffer. Using Deno with WebWorkers # WebWorkers do not have access to elisp functions - you will notice that if you attempt to use the lisp object you will get an error that it is not defined. However, WebWorkers do have full access to Deno. The recommended usage for WebWorkers is to Identify the slow or blocking portion of your existing or new elisp code Write your webworker to perform the operation and data manipulation and translate the results to a format that elisp will be able to use. I.e. if you want to walk a directory and return all files ending in \"foo\", perform that logic and construct the array on your WebWorker, and send that array to your main thread via sendMessage. Once your mainThread receives that information, you can call your elisp code to display it. It's recommended to avoid WebWorkers for pure I/O or subprocess operations. Instead, use Deno's built in async I/O capabilities outlined in their documentation . You should use WebWorkers when you have significant calculation or operation to perform on the returned data. An example would be: Don't use a webworker to just run git status . You may want to use a WebWorker if you are running git status, opening each file, and performing complex logic on each line of the file, and then returning a list of strings back to lisp for display. You can see their example for spawning a subprocess to see what the platform is capable of. This can allow for things normally handled by tramp -> you could ssh into a box, get a list of files, and then actually perform scripting logic on that result prior to passing it back to lisp for display. Current elisp can handle about half of that, however once ssh, or whatever process returns those results, any logic on the subprocesses output will be blocking the editor. You can also reference this module as another example.","title":"Advanced features"},{"location":"js/adv-features/#advanced-features","text":"This section assumes you have completed \"Getting Started\" and \"Using Deno\" and have a basic familiarization with the elisp JavaScript API.","title":"Advanced Features"},{"location":"js/adv-features/#webworkers","text":"One of the big draws of emacs-ng is parallel scripting. You may or may not be familiar with the fact that vanilla emacs had lisp threads that you could create via (make-thread FUNC) . Lisp threads were concurrent, but not parallel. In addition to that limitation, lisp threads that were not the 'main thread' would only execute at specific times during emacs event loop. WebWorkers give you parallelism. Under the hood, they are spinning up a Rust std::thread (in C terms, this is analogous to a pthread). Due to this, there is a limited interface to exchange data between threads.","title":"WebWorkers"},{"location":"js/adv-features/#building-up-the-webworker","text":"We will start by creating the JavaScript our WebWorker will execute. Create a file named \"web-worker.js\" and insert the following: self . onmessage = ( input ) => { let { message } = input . data ; message += \" And My Axe \" ; self . postMessage ({ message }); self . close (); }; WebWorkers communicate via two special functions, postMessage and onmessage. onmessage will be called once our parent thread sends us a message, while we can use postMessage to send data back to the main thread. This example is appending a string to its input and handing that back to the main thread. We also called the close function, meaning that this WebWorker is \"one and done\", and will shut down once it has performed this operation a single time. If we did not call close, the worker would stay alive and await additional messages.","title":"Building up the WebWorker"},{"location":"js/adv-features/#using-our-webworker","text":"Create a new file with the following code: declare var lisp : any ; const worker = new Worker ( new URL ( \"web-worker.js\" , import . meta . url ). href , { type : \"module\" , deno : true , }); worker . onmessage = ( output ) => { const { message } = output . data ; // This is safe because our callback is back // on the main thread. lisp . print ( message ); } worker . postMessage ({ message : \"You have my sword .... \" }); This code spins up our WebWorker and passes it a message. Running this code with eval-ts-buffer should print a reference to a cool little obscure movie in your minibuffer.","title":"Using our WebWorker"},{"location":"js/adv-features/#using-deno-with-webworkers","text":"WebWorkers do not have access to elisp functions - you will notice that if you attempt to use the lisp object you will get an error that it is not defined. However, WebWorkers do have full access to Deno. The recommended usage for WebWorkers is to Identify the slow or blocking portion of your existing or new elisp code Write your webworker to perform the operation and data manipulation and translate the results to a format that elisp will be able to use. I.e. if you want to walk a directory and return all files ending in \"foo\", perform that logic and construct the array on your WebWorker, and send that array to your main thread via sendMessage. Once your mainThread receives that information, you can call your elisp code to display it. It's recommended to avoid WebWorkers for pure I/O or subprocess operations. Instead, use Deno's built in async I/O capabilities outlined in their documentation . You should use WebWorkers when you have significant calculation or operation to perform on the returned data. An example would be: Don't use a webworker to just run git status . You may want to use a WebWorker if you are running git status, opening each file, and performing complex logic on each line of the file, and then returning a list of strings back to lisp for display. You can see their example for spawning a subprocess to see what the platform is capable of. This can allow for things normally handled by tramp -> you could ssh into a box, get a list of files, and then actually perform scripting logic on that result prior to passing it back to lisp for display. Current elisp can handle about half of that, however once ssh, or whatever process returns those results, any logic on the subprocesses output will be blocking the editor. You can also reference this module as another example.","title":"Using Deno with WebWorkers"},{"location":"js/architecture/","text":"Architecture # The purpose of this document is to outline how emacs-ng's native changes work. JavaScript # The majority of JavaScript related code is located in rust_src/src/javascript.rs. There is a lot to unpack on this file, but we will attempt to unpack the core concepts: EmacsMainJsRuntime # This is a thread local singleton containing the JavaScript runtime and associated state. The two major fields in this struct are the Tokio runtime and the MainWorker. The choice to make this a thread local struct was due to the fact that Emacs has the ability to spawn threads via (make-thread), but v8 isolates do not like being shared between threads without using a specific API. We lean heavily on Deno's MainWorker class, which is also not designed to be shared between threads, so we made the choice to just stay threadlocal. The oddity here is that in the case of calling make-thread, the lisp threads will have their lisp variables will shared, but not their JavaScript variables. Tokio # The Tokio runtime is what is driving all JavaScript Async I/O and timers. Tokio maintains its own threadpool on which tasks are enqueued. This is all \"behind the scenes\" to any emacs-ng code. The system is designed with the assumption that emacs-ng code will not have to call tokio::spawn or tokio::spawn_blocking. MainWorker # The \"MainWorker\" is a Deno concept. It encapsulates Deno's module loader, file cache, and most importantly, the \"JsRuntime\". The JsRuntime encapsules the v8::Isolate, which can be described as the true, actual \"JavaScript Runtime\". Interfacing with the v8::Isolate is ultimately how JavaScript will be run. There are few instances where we call execute using the isolate directly. The MainWorker has some key restrictions included by design - if you have a top level module promise rejection, the Worker will panic upon the next attempt to execute JavaScript. The worker cannot handle cases where execute is called \"deep\" within the callstack. Meaning that if I call lisp -> javascript -> lisp -> JavaScript, I cannot depend on the Worker's execute method. We will have an entire section dedicated to that fact. The EmacsMainJsRuntime is thread local because of the MainWorker - it is not Send nor Sync. The v8::Isolate, which is contained within the MainWorker, cannot be shared between threads by design. A quirk of this is that if you spawn a lisp thread, it has its own JavaScript runtime. This means that while the thread shares lisp globals, it does not share javaScript globals. Event Loop # A core concept to JavaScript is the event loop. All JavaScript invocations in emacs-ng start with a call to \"run_module\". run_module will eventually call into the MainWorkers execute method, which in turn calls into the v8::Isolate's execute method - are you starting to see the pattern? Once you call run_module, you will begin to execute JavaScript. Doing so may enqueue async events, like setTimeout, fetch, etc. These are all things that Deno calls async_ops or sync_ops . Deno provides a system to manage ops. We have a policy that emacs-ng will not add ops, or deal with the ops API. If you want to add native functionality to emacs-ng, you should directly bind the function, like with lisp_callback and friends. As these async events execute in the background, we will poll for their completion. In order to integrate with Emacs program loop, we set a timer, calling a function named js-tick-event-loop . This function is really just a wrapper around Deno's poll_event_loop . For performance reasons, js-tick-event-loop can call poll_event_loop multiple times. The user does have control of this behavior via js_set_tick_rate . In general, we want to give the user full control of emacs-ng, including how the JavaScript environment is configured. Proxies and Garbage Collector (GC) Interoperability # JavaScript has a concept of a 'Proxy' object, which we use in emacs-ng, however this section is about our JS <--> elisp marshaling. We refer to that as 'proxying' within internal documentation. For example, we will discuss how this code actually works: const buffer = lisp . get_buffer_create ( '*foo*' ); lisp . set_current_buffer ( buffer ); lisp . insert ( \"FOOBAR\" ); The Lisp Object # The lisp object is a JavaScript Capital-P Proxy . When you access lisp.get_buffer_create , it returns a function that does something like: (... args ) => lisp_invoke ( 'get-buffer-create' ... args ); lisp_invoke is a native function defined in javascript.rs. It's a wrapper around ffuncall with logic for object translation. It will also catch any lisp errors and translates them to JavaScript errors. lisp_invoke returns a JavaScript object - but this is a 'special object' in that it may have an internal field. Internal fields are data objects that are contained within JavaScript objects that are not accessible by the user. This is because LispObjects are just pointers - if we let the user alter pointers, they could cause SEGFAULTS or read arbitrary memory. If we can parse the result of lisp_invoke as JSON, we do not proxy it. Instead we return the JavaScript equivalent (i.e. a JS string or number). However, if we cannot convert it to JSON, like in the case of a buffer, we return a proxy. Functions (i.e. (lambda () (...)) ) are another special case where additional logic is employed. lisp_invoke also works in reverse when being called, it will unproxy the arguments it is passed in order to further pass them to ffuncall . We expose a special function called is_proxy in order to tell if an object is a proxy. GC Usage # When we create a proxy, we need to properly manage it in the lisp garbage collector. We do not want lisp to GC an object out from underneath us. In order to do this, we need to make two considerations for each direction of JS -> Lisp and Lisp -> JS To prevent the lisp GC from removing objects that JS has a valid reference to, we include them in a special cons called the js-retain-map . The user does not have direct access to this object. Allowing them to access this cons would allow mutation in a way that could lead to use after free bugs. When a proxy is created in JavaScript, we create a special JavaScript object called a WeakRef . This is documented here . Once an object has no outstanding references (besides the WeakRef itself) the WeakRef will return undefined once accessed. We maintain a global array of WeakRefs for all proxies that we sweep every time lisp performs its garbage collection. We map this array to the js-retain-map . The end result is that if you have an object in JavaScript that is a proxy, it will always be valid. elisp does not have proxies, it will only receive valid lisp objects from JavaScript, so this problem does not exist in the opposite direction. Lambda Usage # Users will notice that lambas will auto convert between JS and elisp. Example: lisp.run_with_timer ( 1 , lisp.symbols.nil, () => console.log ( 'This works... ' )) ; How does this work under the hood? When we go to create a proxy, we will test if that object is a function. If so, we will include it in another special array for functions. We will then create the following lisp object representing that lambda: ( lambda ( &REST ) ( js--reenter INDEX ( make-finalizer ( lambda () js--clear INDEX )) REST )) Index is hard-coded per lambda object, i.e. js--reenter 1 ... . Calling this lambda will call js--reenter, which is just calling the function in the array at INDEX. In order to ensure that function will be garbage collected, we add a lisp finalizer, which will clear the lambda from the array upon GC.","title":"Architecture"},{"location":"js/architecture/#architecture","text":"The purpose of this document is to outline how emacs-ng's native changes work.","title":"Architecture"},{"location":"js/architecture/#javascript","text":"The majority of JavaScript related code is located in rust_src/src/javascript.rs. There is a lot to unpack on this file, but we will attempt to unpack the core concepts:","title":"JavaScript"},{"location":"js/architecture/#emacsmainjsruntime","text":"This is a thread local singleton containing the JavaScript runtime and associated state. The two major fields in this struct are the Tokio runtime and the MainWorker. The choice to make this a thread local struct was due to the fact that Emacs has the ability to spawn threads via (make-thread), but v8 isolates do not like being shared between threads without using a specific API. We lean heavily on Deno's MainWorker class, which is also not designed to be shared between threads, so we made the choice to just stay threadlocal. The oddity here is that in the case of calling make-thread, the lisp threads will have their lisp variables will shared, but not their JavaScript variables.","title":"EmacsMainJsRuntime"},{"location":"js/architecture/#tokio","text":"The Tokio runtime is what is driving all JavaScript Async I/O and timers. Tokio maintains its own threadpool on which tasks are enqueued. This is all \"behind the scenes\" to any emacs-ng code. The system is designed with the assumption that emacs-ng code will not have to call tokio::spawn or tokio::spawn_blocking.","title":"Tokio"},{"location":"js/architecture/#mainworker","text":"The \"MainWorker\" is a Deno concept. It encapsulates Deno's module loader, file cache, and most importantly, the \"JsRuntime\". The JsRuntime encapsules the v8::Isolate, which can be described as the true, actual \"JavaScript Runtime\". Interfacing with the v8::Isolate is ultimately how JavaScript will be run. There are few instances where we call execute using the isolate directly. The MainWorker has some key restrictions included by design - if you have a top level module promise rejection, the Worker will panic upon the next attempt to execute JavaScript. The worker cannot handle cases where execute is called \"deep\" within the callstack. Meaning that if I call lisp -> javascript -> lisp -> JavaScript, I cannot depend on the Worker's execute method. We will have an entire section dedicated to that fact. The EmacsMainJsRuntime is thread local because of the MainWorker - it is not Send nor Sync. The v8::Isolate, which is contained within the MainWorker, cannot be shared between threads by design. A quirk of this is that if you spawn a lisp thread, it has its own JavaScript runtime. This means that while the thread shares lisp globals, it does not share javaScript globals.","title":"MainWorker"},{"location":"js/architecture/#event-loop","text":"A core concept to JavaScript is the event loop. All JavaScript invocations in emacs-ng start with a call to \"run_module\". run_module will eventually call into the MainWorkers execute method, which in turn calls into the v8::Isolate's execute method - are you starting to see the pattern? Once you call run_module, you will begin to execute JavaScript. Doing so may enqueue async events, like setTimeout, fetch, etc. These are all things that Deno calls async_ops or sync_ops . Deno provides a system to manage ops. We have a policy that emacs-ng will not add ops, or deal with the ops API. If you want to add native functionality to emacs-ng, you should directly bind the function, like with lisp_callback and friends. As these async events execute in the background, we will poll for their completion. In order to integrate with Emacs program loop, we set a timer, calling a function named js-tick-event-loop . This function is really just a wrapper around Deno's poll_event_loop . For performance reasons, js-tick-event-loop can call poll_event_loop multiple times. The user does have control of this behavior via js_set_tick_rate . In general, we want to give the user full control of emacs-ng, including how the JavaScript environment is configured.","title":"Event Loop"},{"location":"js/architecture/#proxies-and-garbage-collector-gc-interoperability","text":"JavaScript has a concept of a 'Proxy' object, which we use in emacs-ng, however this section is about our JS <--> elisp marshaling. We refer to that as 'proxying' within internal documentation. For example, we will discuss how this code actually works: const buffer = lisp . get_buffer_create ( '*foo*' ); lisp . set_current_buffer ( buffer ); lisp . insert ( \"FOOBAR\" );","title":"Proxies and Garbage Collector (GC) Interoperability"},{"location":"js/architecture/#the-lisp-object","text":"The lisp object is a JavaScript Capital-P Proxy . When you access lisp.get_buffer_create , it returns a function that does something like: (... args ) => lisp_invoke ( 'get-buffer-create' ... args ); lisp_invoke is a native function defined in javascript.rs. It's a wrapper around ffuncall with logic for object translation. It will also catch any lisp errors and translates them to JavaScript errors. lisp_invoke returns a JavaScript object - but this is a 'special object' in that it may have an internal field. Internal fields are data objects that are contained within JavaScript objects that are not accessible by the user. This is because LispObjects are just pointers - if we let the user alter pointers, they could cause SEGFAULTS or read arbitrary memory. If we can parse the result of lisp_invoke as JSON, we do not proxy it. Instead we return the JavaScript equivalent (i.e. a JS string or number). However, if we cannot convert it to JSON, like in the case of a buffer, we return a proxy. Functions (i.e. (lambda () (...)) ) are another special case where additional logic is employed. lisp_invoke also works in reverse when being called, it will unproxy the arguments it is passed in order to further pass them to ffuncall . We expose a special function called is_proxy in order to tell if an object is a proxy.","title":"The Lisp Object"},{"location":"js/architecture/#gc-usage","text":"When we create a proxy, we need to properly manage it in the lisp garbage collector. We do not want lisp to GC an object out from underneath us. In order to do this, we need to make two considerations for each direction of JS -> Lisp and Lisp -> JS To prevent the lisp GC from removing objects that JS has a valid reference to, we include them in a special cons called the js-retain-map . The user does not have direct access to this object. Allowing them to access this cons would allow mutation in a way that could lead to use after free bugs. When a proxy is created in JavaScript, we create a special JavaScript object called a WeakRef . This is documented here . Once an object has no outstanding references (besides the WeakRef itself) the WeakRef will return undefined once accessed. We maintain a global array of WeakRefs for all proxies that we sweep every time lisp performs its garbage collection. We map this array to the js-retain-map . The end result is that if you have an object in JavaScript that is a proxy, it will always be valid. elisp does not have proxies, it will only receive valid lisp objects from JavaScript, so this problem does not exist in the opposite direction.","title":"GC Usage"},{"location":"js/architecture/#lambda-usage","text":"Users will notice that lambas will auto convert between JS and elisp. Example: lisp.run_with_timer ( 1 , lisp.symbols.nil, () => console.log ( 'This works... ' )) ; How does this work under the hood? When we go to create a proxy, we will test if that object is a function. If so, we will include it in another special array for functions. We will then create the following lisp object representing that lambda: ( lambda ( &REST ) ( js--reenter INDEX ( make-finalizer ( lambda () js--clear INDEX )) REST )) Index is hard-coded per lambda object, i.e. js--reenter 1 ... . Calling this lambda will call js--reenter, which is just calling the function in the array at INDEX. In order to ensure that function will be garbage collected, we add a lisp finalizer, which will clear the lambda from the array upon GC.","title":"Lambda Usage"},{"location":"js/getting-started/","text":"Getting Started # Note This feature is currently outdated and been disabled. However there is this to bring it back. This is an introduction for building applications and hacking on emacs-ng using JavaScript. emacs-ng supports both TypeScript and JavaScript, so these examples will feature both languages. This guide uses emacs terminology for key binds, i.e. C-x means holding down control and pressing x. M-x means holding Alt and pressing x (unless you are on a system where Escape is the 'Meta' key). C-x C-f means hold down control, press x, keep holding control, press f. Requirements # You will need Rust installed . The file rust-toolchain indicates the version that gets installed. This happens automatically, so don't override the toolchain manually. IMPORTANT: Whenever the toolchain updates, you have to reinstall rustfmt manually. You also need a working installation of emacs-ng. You can either build it or install a precompiled binary distribution from our CI . Running JavaScript # Type in the following line and press C-j ( eval-js \"lisp.print('hello world!')\" ) This will display \"hello world\" in your display area (along with 'nil' - we will get to that). This is an anonymous javascript evaluation. Before we go further, let\u2019s make an environment for working with our new scripting language. We have multiple ways to run JavaScript. Let us open a new TypeScript file by creating a file name \"basic.ts\" in our current directory. It will look something like: declare var lisp : any ; let x : string = \"Hello TypeScript\" ; lisp . print ( x ); Now we go back to *scratch* and run ( eval-js-file \"./basic.ts\" ) This is a relative filepath, and it works off of emacs current working directory (cwd). If you are in doubt to what emacs cwd is, just run the lisp function (pwd) in the lisp scratchpad You should see \"Hello TypeScript\" printed. All of the eval-js* functions return nil. If you want to use a calculated value from JavaScript in Lisp, you should use eval-js-literally . If you are looking for something like eval-expression , which is normally bound to M-:, you should use eval-js-expression . It accepts the same arguments as eval-expression , except with the first argument being a JavaScript expression, and behaves very similarly to eval-expression . It will also inserts the results into values just like eval-expression . eval-js-literally and eval-js-expression do not work with TypeScript at this time. Iteration # Now that we have our TypeScript file, let us get out of lisp and work purely in TypeScript. Open \"basic.ts\" by pressing C-x C-f and open \"basic.ts\". Press M-x and enter \"eval-ts-buffer\". This will evaluate the current contents of your buffer as typescript. You should see \"Hello Typescript\" print in your minibuffer. From now on, this will be our preferred way to iterate. If you do not want to evaluate the entire buffer, you can press C-space, highlight a region of code, and press M-x eval-ts-region. Try that with this code and see what happens: let y : string = 3 ; You will see the following error in your minibuffer: TS2322 [ ERROR ] : Type 'number' is not assignable to type 'string' . let y: string = 3 ; ^ at file:///home/user/ $anon$lisp$91610855413 .ts:1:5TS2322 It's important to understand that your TypeScript code is compiled prior to execution - unlike standard JavaScript which is evaluated until you encounter a runtime error. Within emacs-ng, that means that your code isn't executed if you have a type error in TypeScript. If these typescript examples are taking a long time to evaluate, it's likely due to the processing power required to compile everything. If you want to turn off TypeScript typechecking for the remainder of the examples, you can run: ( js-cleanup ) ( js-initialize :no-check t ) This code will cleanup your current JS environment and re-initialize it with TypeScript type checking disabled. If you do not care about the type checking that TypeScript offers, or your computer struggles with the cost of compiling, you can add (js-initialize :no-check t) to your init.el. Let's stop printing to the minibuffer, and instead start pushing our results into buffers. Let's start by something simple: make a network call and dump the results into a buffer. Buffers # First thing first, let's make our network call. In order to do this, we will use our built-in Deno APIs . Deno implements fetch , which looks something like this: declare var lisp : any ; fetch ( \"https://api.github.com/users/denoland\" ) . then ( response => response . json ()) . catch ( e => lisp . print ( JSON . stringify ( e ))); fetch is a common API documented here . It returns a Promise , which is a tool we will use to manage async I/O operations. Here, the network call isn't blocking, and it managed by our Rust runtime called Tokio. The .then is saying that once this promise is resolved, execute the next function in the chain. We have added a .catch , which will be executed if fetch errors. NOTE: If you have an unhandled toplevel promise rejection, the JavaScript runtime will RESET. You should always have a toplevel promise handler within any emacs-ng code. We will see the way we can interface with lisp error handling further on. However we want to put this response into a buffer. In order to do this, we will extend our .then chain, like so declare var lisp : any ; fetch ( \"https://api.github.com/users/denoland\" ) . then ( response => response . json ()) . then (( data ) => { const buffer = lisp . get_buffer_create ( \"TypeScript Buffer\" ); lisp . with_current_buffer ( buffer , () => lisp . insert ( JSON . stringify ( data ))); }) . catch ( e => lisp . print ( JSON . stringify ( e ))); Wait for the network call to resolve, and navigate to \"TypeScript Buffer\" via C-x b and typing in \"TypeScript Buffer\", or pressing C-x C-b and selecting our buffer from the buffer list. By now, you may be wondering about this lisp object, and how we are able to get references to lisp objects from JavaScript. Our next example illustrates this. Filewatching # Let's write an async filewatch that logs changes to a directory into a buffer, with a little extra data. In order to do this, we will use Deno's standard library. Deno has built in functions like fetch , along with a robust standard library that you need to import. That will look like this: declare var lisp : any ; // This will allow us to write const insertIntoTypeScriptBuffer = ( str : string ) => { const buffer = lisp . get_buffer_create ( \"TypeScript Filewatching\" ); lisp . with_current_buffer ( buffer , () => lisp . insert ( ` ${ str } \\n` )); }; async function watch ( dir : string ) { const watcher = Deno . watchFs ( dir ); let i = 0 ; for await ( const event of watcher ) { i += 1 ; if ( i > 5 ) break ; insertIntoTypeScriptBuffer ( JSON . stringify ( event )); } } watch ( '.' ) This example is built to only record 5 events prior to ending itself. You can write whatever logic you would like for ending your filewatcher. Running touch foo.ts in your current directory should yield something like the following in the \"TypeScript Filewatching\" buffer { \"kind\" : \"create\" , \"paths\" :[ \"/home/user/./foo.ts\" ]} { \"kind\" : \"modify\" , \"paths\" :[ \"/home/user/./foo.ts\" ]} { \"kind\" : \"access\" , \"paths\" :[ \"/home/user/./foo.ts\" ]} Deno has further documentation on this . Note that these events can differ per operating system. Key takeaways here - all of the TypeScript written above is executed on the Main elisp thread - there are no race conditions with lisp here. Even though the filewatcher is async, it calls back onto the main thread when it has data. Multithreaded scripting is possible and will be covered later. Modules # Now let's look at our tools for importing code. emacs-ng supports ES6 modules . emacs-ng does not support node's require syntax. See the \"Using Deno\" section for more information on modules. Let's create a submodule for our main program. Create a file named \"mod-sub.js\": export function generateRandomNumber () { return 4 ; } Now in our main file, we can add the following to the top of our file: import { generateRandomNumber } from \"./mod-sub.js\" ; declare var lisp : any ; lisp . print ( generateRandomNumber ()); Even though our module is TypeScript, we can still import plain old JavaScript. It's important to note that ES6 modules supposed to be immutable. What does that mean? If we were to edit mod-js to include the following: export function generateRandomNumber () { return 4 ; } lisp . print ( generateRandomNumber ()); We see that in addition to exporting a function, we execute code with side-effects (printing). Those side-effects only happen once . If I import mod-sub multiple times, I will only ever see \"4\" printed once. Another important note is that this rule does not apply to any top-level module you execute . Meaning that if you call (eval-js-file \"./basic.ts\") multiple times, your code is executed every single time, however your dependencies are only executed once. This is by design. If you want to break this, you can append a number to your dependency and use so-called dynamic importing , like so: declare var lisp : any ; let timestamp : number = Date . now (); const { generateRandomNumber } = await import ( `./mod-sub.js# ${ timestamp } ` ); lisp . print ( generateRandomNumber ()); This can be useful if you are a module developer and you want to iterate on your modules within emacs-ng. It is recommended that you do not ship your modules using this pattern, as it will not cache results properly and lead to a sub-optimal user experience. Your imports should aim to not have side effects, and instead should only export functions or variables to be used by your main module. The intended use of Dynamic Importing is to allow you to have condition imports, like so: if ( myCondition ) { const { func } = await import ( 'example-mod.js' ); func (); } Lisp Interaction # The lisp object is magic - it has (almost) all lisp functions defined on it, including any functions defined in your custom packages. If you can invoke it via (funcall ....) , you can call it via the lisp object if you change - for _ . For example: (with-current-buffer (get-buffer-create \"BUFFER\") (lambda () (insert \"DATA\"))) becomes lisp . with_current_buffer ( lisp . get_buffer_create ( \"BUFFER\" ), () => lisp . insert ( \"DATA\" )); We have implementations of common macros like with-current-buffer . If you find that a certain common macro doesn't work, you can report it to the project's maintainers and they will implement it, however what they are doing isn't magic - they are just calling eval on your whatever macro you want to invoke from JavaScript. lisp . eval ( lisp . list ( lisp . symbols . with_current_buffer , arg1 , arg2 )); You can override the behavior of the lisp object via the special object specialForms , which looks like lisp . specialForms . with_current_buffer = myWithCurrentBufferFunction ; This overrides JavaScript's implementation of with_current_buffer without touching lisp's implementation. Let's discuss working with lisp more: Lisp Primitives # Primitives (Number, String, Boolean) are automatically translated when calling lisp functions lisp . my_function ( 1.0 , 2 , \"MYSTRING\" , false ) If you need to access a symbol or keyword, you will use the symbol keyword objects const mySymbol = lisp . symbols . foo ; // foo const myKeyword = lisp . keywords . foo ; // :foo You can create more complex objects via the make object const hashtable = lisp . make . hashtable ({ x : 3 , y : 4 }); const alist = lisp . make . alist ({ x : lisp.symbols.bar , y : \"String\" }); const plist = lisp . make . plist ({ zz : 19 , zx : false }); const array = lisp . make . array ([ 1 , 2 , 3 , 4 , 5 ]); const list = lisp . make . list ([ 1 , 5 , \"String\" , lisp . symbols . x ]); const lstring = lisp . make . string ( \"MyString\" ); Errors # If a lisp function would trigger (error ...) in lisp, it will throw an error in javascript. An example: try { lisp . cons (); // No arguments } catch ( e ) { lisp . print ( JSON . stringify ( e )); } Defining Lisp Functions # We can also define functions that can be called via lisp. We will use defun to accomplish this: declare var lisp : any ; const insertIntoTypeScriptBuffer = ( str : string ) => { const buffer = lisp . get_buffer_create ( \"TypeScript Filewatching\" ); lisp . with_current_buffer ( buffer , () => lisp . insert ( str )); }; lisp . defun ({ name : \"my-function\" , docString : \"My Example Function\" , interactive : true , args : \"MInput\" , func : ( str : string ) => insertIntoTypeScriptBuffer ( str ) }); This defines a lisp function named my-function . We can call this function from lisp (my-function STRING) , in JavaScript via lisp.my_function(STRING) , or call it interactively. That means that within the editor if we press M-x and type \"my-function\", we can invoke the function. It will then perform our JavaScript action, which is to insert whatever text we enter into our TypeScript Buffer. Conclusion # This covers the basic of calling lisp functions and I/O using Deno. Together using these tools you can already build powerful apps, or allow emacs-ng to perform actions. In our next series we will cover more advanced topics like Threading and WebASM.","title":"Getting started"},{"location":"js/getting-started/#getting-started","text":"Note This feature is currently outdated and been disabled. However there is this to bring it back. This is an introduction for building applications and hacking on emacs-ng using JavaScript. emacs-ng supports both TypeScript and JavaScript, so these examples will feature both languages. This guide uses emacs terminology for key binds, i.e. C-x means holding down control and pressing x. M-x means holding Alt and pressing x (unless you are on a system where Escape is the 'Meta' key). C-x C-f means hold down control, press x, keep holding control, press f.","title":"Getting Started"},{"location":"js/getting-started/#requirements","text":"You will need Rust installed . The file rust-toolchain indicates the version that gets installed. This happens automatically, so don't override the toolchain manually. IMPORTANT: Whenever the toolchain updates, you have to reinstall rustfmt manually. You also need a working installation of emacs-ng. You can either build it or install a precompiled binary distribution from our CI .","title":"Requirements"},{"location":"js/getting-started/#running-javascript","text":"Type in the following line and press C-j ( eval-js \"lisp.print('hello world!')\" ) This will display \"hello world\" in your display area (along with 'nil' - we will get to that). This is an anonymous javascript evaluation. Before we go further, let\u2019s make an environment for working with our new scripting language. We have multiple ways to run JavaScript. Let us open a new TypeScript file by creating a file name \"basic.ts\" in our current directory. It will look something like: declare var lisp : any ; let x : string = \"Hello TypeScript\" ; lisp . print ( x ); Now we go back to *scratch* and run ( eval-js-file \"./basic.ts\" ) This is a relative filepath, and it works off of emacs current working directory (cwd). If you are in doubt to what emacs cwd is, just run the lisp function (pwd) in the lisp scratchpad You should see \"Hello TypeScript\" printed. All of the eval-js* functions return nil. If you want to use a calculated value from JavaScript in Lisp, you should use eval-js-literally . If you are looking for something like eval-expression , which is normally bound to M-:, you should use eval-js-expression . It accepts the same arguments as eval-expression , except with the first argument being a JavaScript expression, and behaves very similarly to eval-expression . It will also inserts the results into values just like eval-expression . eval-js-literally and eval-js-expression do not work with TypeScript at this time.","title":"Running JavaScript"},{"location":"js/getting-started/#iteration","text":"Now that we have our TypeScript file, let us get out of lisp and work purely in TypeScript. Open \"basic.ts\" by pressing C-x C-f and open \"basic.ts\". Press M-x and enter \"eval-ts-buffer\". This will evaluate the current contents of your buffer as typescript. You should see \"Hello Typescript\" print in your minibuffer. From now on, this will be our preferred way to iterate. If you do not want to evaluate the entire buffer, you can press C-space, highlight a region of code, and press M-x eval-ts-region. Try that with this code and see what happens: let y : string = 3 ; You will see the following error in your minibuffer: TS2322 [ ERROR ] : Type 'number' is not assignable to type 'string' . let y: string = 3 ; ^ at file:///home/user/ $anon$lisp$91610855413 .ts:1:5TS2322 It's important to understand that your TypeScript code is compiled prior to execution - unlike standard JavaScript which is evaluated until you encounter a runtime error. Within emacs-ng, that means that your code isn't executed if you have a type error in TypeScript. If these typescript examples are taking a long time to evaluate, it's likely due to the processing power required to compile everything. If you want to turn off TypeScript typechecking for the remainder of the examples, you can run: ( js-cleanup ) ( js-initialize :no-check t ) This code will cleanup your current JS environment and re-initialize it with TypeScript type checking disabled. If you do not care about the type checking that TypeScript offers, or your computer struggles with the cost of compiling, you can add (js-initialize :no-check t) to your init.el. Let's stop printing to the minibuffer, and instead start pushing our results into buffers. Let's start by something simple: make a network call and dump the results into a buffer.","title":"Iteration"},{"location":"js/getting-started/#buffers","text":"First thing first, let's make our network call. In order to do this, we will use our built-in Deno APIs . Deno implements fetch , which looks something like this: declare var lisp : any ; fetch ( \"https://api.github.com/users/denoland\" ) . then ( response => response . json ()) . catch ( e => lisp . print ( JSON . stringify ( e ))); fetch is a common API documented here . It returns a Promise , which is a tool we will use to manage async I/O operations. Here, the network call isn't blocking, and it managed by our Rust runtime called Tokio. The .then is saying that once this promise is resolved, execute the next function in the chain. We have added a .catch , which will be executed if fetch errors. NOTE: If you have an unhandled toplevel promise rejection, the JavaScript runtime will RESET. You should always have a toplevel promise handler within any emacs-ng code. We will see the way we can interface with lisp error handling further on. However we want to put this response into a buffer. In order to do this, we will extend our .then chain, like so declare var lisp : any ; fetch ( \"https://api.github.com/users/denoland\" ) . then ( response => response . json ()) . then (( data ) => { const buffer = lisp . get_buffer_create ( \"TypeScript Buffer\" ); lisp . with_current_buffer ( buffer , () => lisp . insert ( JSON . stringify ( data ))); }) . catch ( e => lisp . print ( JSON . stringify ( e ))); Wait for the network call to resolve, and navigate to \"TypeScript Buffer\" via C-x b and typing in \"TypeScript Buffer\", or pressing C-x C-b and selecting our buffer from the buffer list. By now, you may be wondering about this lisp object, and how we are able to get references to lisp objects from JavaScript. Our next example illustrates this.","title":"Buffers"},{"location":"js/getting-started/#filewatching","text":"Let's write an async filewatch that logs changes to a directory into a buffer, with a little extra data. In order to do this, we will use Deno's standard library. Deno has built in functions like fetch , along with a robust standard library that you need to import. That will look like this: declare var lisp : any ; // This will allow us to write const insertIntoTypeScriptBuffer = ( str : string ) => { const buffer = lisp . get_buffer_create ( \"TypeScript Filewatching\" ); lisp . with_current_buffer ( buffer , () => lisp . insert ( ` ${ str } \\n` )); }; async function watch ( dir : string ) { const watcher = Deno . watchFs ( dir ); let i = 0 ; for await ( const event of watcher ) { i += 1 ; if ( i > 5 ) break ; insertIntoTypeScriptBuffer ( JSON . stringify ( event )); } } watch ( '.' ) This example is built to only record 5 events prior to ending itself. You can write whatever logic you would like for ending your filewatcher. Running touch foo.ts in your current directory should yield something like the following in the \"TypeScript Filewatching\" buffer { \"kind\" : \"create\" , \"paths\" :[ \"/home/user/./foo.ts\" ]} { \"kind\" : \"modify\" , \"paths\" :[ \"/home/user/./foo.ts\" ]} { \"kind\" : \"access\" , \"paths\" :[ \"/home/user/./foo.ts\" ]} Deno has further documentation on this . Note that these events can differ per operating system. Key takeaways here - all of the TypeScript written above is executed on the Main elisp thread - there are no race conditions with lisp here. Even though the filewatcher is async, it calls back onto the main thread when it has data. Multithreaded scripting is possible and will be covered later.","title":"Filewatching"},{"location":"js/getting-started/#modules","text":"Now let's look at our tools for importing code. emacs-ng supports ES6 modules . emacs-ng does not support node's require syntax. See the \"Using Deno\" section for more information on modules. Let's create a submodule for our main program. Create a file named \"mod-sub.js\": export function generateRandomNumber () { return 4 ; } Now in our main file, we can add the following to the top of our file: import { generateRandomNumber } from \"./mod-sub.js\" ; declare var lisp : any ; lisp . print ( generateRandomNumber ()); Even though our module is TypeScript, we can still import plain old JavaScript. It's important to note that ES6 modules supposed to be immutable. What does that mean? If we were to edit mod-js to include the following: export function generateRandomNumber () { return 4 ; } lisp . print ( generateRandomNumber ()); We see that in addition to exporting a function, we execute code with side-effects (printing). Those side-effects only happen once . If I import mod-sub multiple times, I will only ever see \"4\" printed once. Another important note is that this rule does not apply to any top-level module you execute . Meaning that if you call (eval-js-file \"./basic.ts\") multiple times, your code is executed every single time, however your dependencies are only executed once. This is by design. If you want to break this, you can append a number to your dependency and use so-called dynamic importing , like so: declare var lisp : any ; let timestamp : number = Date . now (); const { generateRandomNumber } = await import ( `./mod-sub.js# ${ timestamp } ` ); lisp . print ( generateRandomNumber ()); This can be useful if you are a module developer and you want to iterate on your modules within emacs-ng. It is recommended that you do not ship your modules using this pattern, as it will not cache results properly and lead to a sub-optimal user experience. Your imports should aim to not have side effects, and instead should only export functions or variables to be used by your main module. The intended use of Dynamic Importing is to allow you to have condition imports, like so: if ( myCondition ) { const { func } = await import ( 'example-mod.js' ); func (); }","title":"Modules"},{"location":"js/getting-started/#lisp-interaction","text":"The lisp object is magic - it has (almost) all lisp functions defined on it, including any functions defined in your custom packages. If you can invoke it via (funcall ....) , you can call it via the lisp object if you change - for _ . For example: (with-current-buffer (get-buffer-create \"BUFFER\") (lambda () (insert \"DATA\"))) becomes lisp . with_current_buffer ( lisp . get_buffer_create ( \"BUFFER\" ), () => lisp . insert ( \"DATA\" )); We have implementations of common macros like with-current-buffer . If you find that a certain common macro doesn't work, you can report it to the project's maintainers and they will implement it, however what they are doing isn't magic - they are just calling eval on your whatever macro you want to invoke from JavaScript. lisp . eval ( lisp . list ( lisp . symbols . with_current_buffer , arg1 , arg2 )); You can override the behavior of the lisp object via the special object specialForms , which looks like lisp . specialForms . with_current_buffer = myWithCurrentBufferFunction ; This overrides JavaScript's implementation of with_current_buffer without touching lisp's implementation. Let's discuss working with lisp more:","title":"Lisp Interaction"},{"location":"js/getting-started/#lisp-primitives","text":"Primitives (Number, String, Boolean) are automatically translated when calling lisp functions lisp . my_function ( 1.0 , 2 , \"MYSTRING\" , false ) If you need to access a symbol or keyword, you will use the symbol keyword objects const mySymbol = lisp . symbols . foo ; // foo const myKeyword = lisp . keywords . foo ; // :foo You can create more complex objects via the make object const hashtable = lisp . make . hashtable ({ x : 3 , y : 4 }); const alist = lisp . make . alist ({ x : lisp.symbols.bar , y : \"String\" }); const plist = lisp . make . plist ({ zz : 19 , zx : false }); const array = lisp . make . array ([ 1 , 2 , 3 , 4 , 5 ]); const list = lisp . make . list ([ 1 , 5 , \"String\" , lisp . symbols . x ]); const lstring = lisp . make . string ( \"MyString\" );","title":"Lisp Primitives"},{"location":"js/getting-started/#errors","text":"If a lisp function would trigger (error ...) in lisp, it will throw an error in javascript. An example: try { lisp . cons (); // No arguments } catch ( e ) { lisp . print ( JSON . stringify ( e )); }","title":"Errors"},{"location":"js/getting-started/#defining-lisp-functions","text":"We can also define functions that can be called via lisp. We will use defun to accomplish this: declare var lisp : any ; const insertIntoTypeScriptBuffer = ( str : string ) => { const buffer = lisp . get_buffer_create ( \"TypeScript Filewatching\" ); lisp . with_current_buffer ( buffer , () => lisp . insert ( str )); }; lisp . defun ({ name : \"my-function\" , docString : \"My Example Function\" , interactive : true , args : \"MInput\" , func : ( str : string ) => insertIntoTypeScriptBuffer ( str ) }); This defines a lisp function named my-function . We can call this function from lisp (my-function STRING) , in JavaScript via lisp.my_function(STRING) , or call it interactively. That means that within the editor if we press M-x and type \"my-function\", we can invoke the function. It will then perform our JavaScript action, which is to insert whatever text we enter into our TypeScript Buffer.","title":"Defining Lisp Functions"},{"location":"js/getting-started/#conclusion","text":"This covers the basic of calling lisp functions and I/O using Deno. Together using these tools you can already build powerful apps, or allow emacs-ng to perform actions. In our next series we will cover more advanced topics like Threading and WebASM.","title":"Conclusion"},{"location":"js/main-features/","text":"Main Features # Javascript # This code is a strictly additive layer, it changes no elisp functionality, and should be able to merge upstream patches cleanly. JavaScript tests can be run by building the editor and executing cd test/js && ../../src/emacs --batch --eval '(deno \"test\" \"--allow-read\" \"--allow-write\" \"main.js\")' . To learn more about JavaScript and TypeScript, check out Getting Started , Using Deno , and Advanced Features Using Async I/O # We expose the async IO functionality included with Deno. Users can fetch data async from their local file system, or the network. They can use that data to interact with the editor. An example would be: const json = fetch ( \"https://api.github.com/users/denoland\" ) . then (( response ) => { return response . json (); }); const txt = Deno . readTextFile ( \"./test.json\" ); Promise . all ([ json , text ]) . then (( data ) => { let buffer = lisp . get_buffer_create ( 'hello' ); const current = lisp . current_buffer (); lisp . set_buffer ( buffer ); lisp . insert ( JSON . stringify ( data [ 0 ])); lisp . insert ( data [ 1 ]); console . log ( lisp . buffer_string ()); lisp . set_buffer ( current ); }); This example assumes you have a json file named test.json in your current directory. WebWorkers and Parallel Scripting # We also support WebWorkers, meaning that you can run JavaScript in separate threads. Note that WebWorkers cannot interact with the lisp VM, however they can use Deno for async I/O. See Advanced Features Web Assembly allows you to perform things normally handled by native libraries with easy distribution. Want to manipulate sqlite3? Use the deno sqlite wasm package import { DB } from \"https://deno.land/x/sqlite@v2.3.2/mod.ts\" ; const db = new DB ( \"test.db\" ); db . query ( \"CREATE TABLE IF NOT EXISTS people (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT)\" ); const name = \"David\" ; db . query ( \"INSERT INTO people (name) VALUES (?)\" , [ name ]); for ( const [ name ] of db . query ( \"SELECT name FROM people\" )) { console . log ( name ); } db . close ();","title":"Basic features"},{"location":"js/main-features/#main-features","text":"","title":"Main Features"},{"location":"js/main-features/#javascript","text":"This code is a strictly additive layer, it changes no elisp functionality, and should be able to merge upstream patches cleanly. JavaScript tests can be run by building the editor and executing cd test/js && ../../src/emacs --batch --eval '(deno \"test\" \"--allow-read\" \"--allow-write\" \"main.js\")' . To learn more about JavaScript and TypeScript, check out Getting Started , Using Deno , and Advanced Features","title":"Javascript"},{"location":"js/main-features/#using-async-io","text":"We expose the async IO functionality included with Deno. Users can fetch data async from their local file system, or the network. They can use that data to interact with the editor. An example would be: const json = fetch ( \"https://api.github.com/users/denoland\" ) . then (( response ) => { return response . json (); }); const txt = Deno . readTextFile ( \"./test.json\" ); Promise . all ([ json , text ]) . then (( data ) => { let buffer = lisp . get_buffer_create ( 'hello' ); const current = lisp . current_buffer (); lisp . set_buffer ( buffer ); lisp . insert ( JSON . stringify ( data [ 0 ])); lisp . insert ( data [ 1 ]); console . log ( lisp . buffer_string ()); lisp . set_buffer ( current ); }); This example assumes you have a json file named test.json in your current directory.","title":"Using Async I/O"},{"location":"js/main-features/#webworkers-and-parallel-scripting","text":"We also support WebWorkers, meaning that you can run JavaScript in separate threads. Note that WebWorkers cannot interact with the lisp VM, however they can use Deno for async I/O. See Advanced Features Web Assembly allows you to perform things normally handled by native libraries with easy distribution. Want to manipulate sqlite3? Use the deno sqlite wasm package import { DB } from \"https://deno.land/x/sqlite@v2.3.2/mod.ts\" ; const db = new DB ( \"test.db\" ); db . query ( \"CREATE TABLE IF NOT EXISTS people (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT)\" ); const name = \"David\" ; db . query ( \"INSERT INTO people (name) VALUES (?)\" , [ name ]); for ( const [ name ] of db . query ( \"SELECT name FROM people\" )) { console . log ( name ); } db . close ();","title":"WebWorkers and Parallel Scripting"},{"location":"js/using-deno/","text":"Using the power of Deno # What is Deno? # Deno is a program that is similar to Node.js, except that it is written in Rust. Both Deno and Node.js were created by the same person: Ryan Dhal. While normally you would invoke Deno via the command line, the emacs-ng project has integrated the Deno runtime into the emacs-ng client directly. Deno is powered by v8, Chrome's Open Source JavaScript engine. A JavaScript engine is more limited than most users realize. For example, utilities like XMLHttpRequest are not provided directly by the JavaScript engine, but are instead provided by a runtime (like your browser). Deno provides interfaces for performing I/O operations like file reads/writes, network operations, and spawning subprocesses. By default, emacs-ng will allow reading, writing, network, and subprocess operations. In Deno's examples, you may see them advise you to pass flags like \"--allow-net\". These are not needed when executing emacs-ng code. If you want to globally disable any of the previously mentioned operations, you can run the following lisp prior to executing JavaScript: (js-initialize :allow-net nil :allow-read nil :allow-write nil :allow-run nil) You can define any combination of the above arguments. See js-initialize's documentation for more information. General Documentation / Standard Library # Deno has excellent documentation . This guide is to give you a basic familiarity with Deno to help you to quickly write applications, but is FAR from all inclusive. Deno maintains a powerful standard library at https://deno.land/std@0.83.0 . Importing a Deno std module is simple: just include this in your JavaScript file/buffer: Credit to https://deno.land/std@0.83.0/fs for the example: import { copy , copySync } from \"https://deno.land/std@0.83.0/fs/mod.ts\" ; copy ( \"./foo\" , \"./bar\" ); // returns a promise copySync ( \"./foo\" , \"./bar\" ); // void copySync ( \"./foo\" , \"./existingFolder\" , { overwrite : true }); The first thing you may notice is that we are importing a URL, not a local filepath. Deno allows you to download dependencies from the network. This file will only be downloaded once and compiled once, and it's results will be cached on your local file system. After initial download, you will use your local filesystem's copy instead of using the network. By default, all Deno API's are asynchronous . Almost all async operations have an alternate version that is synchronous. That means that when you make a call to read a file, or walk a directory, it is returning a Promise. Deno has the naming convention that the synchronous versions all end in \" Sync\". A common point of confusion with emacs-ng is that when you evaluate a file, a buffer, or even an anonymous block in JavaScript via (eval-js) , you are executing your code within a JavaScript module with top level await enabled. What does that mean? If you invoke await within your top level module, you will block the main thread until completion. Looking again with our example above with that in mind: import { copy } from \"https://deno.land/std@0.83.0/fs/mod.ts\" ; // This block the main thread, including lisp execution, until this action is completed await copy ( \"./foo\" , \"./bar\" ); // This does not block JavaScript or Lisp, instead our .then will be executed once // the async action is complete. copy ( \"./foo\" , \"./bar\" ). then (() => console . log ( \"Complete\" )); Remember that async/await in JavaScript is just syntax sugar over Promises. There may be times where you want to use the top level await functionality to block on a promise at a certain time. Distribution # Once you have created your great emacs-ng module, how do you distribute it? Normally you would go through a repository like ELPA or MELPLA. While that is still a possibility, you have a third option, which is Deno's user modules . Navigating to the link below gives you the information on the upload process, but in the author's opinion, it is very simple and streamlined. Once your module is uploaded, you can have your user's include a line similar to this in their init.el (eval-js \"import 'https://deno.land/x/fuzzy_search@0.3.0/mod-fuzzy.js'\") Where instead of fuzzy_search@0.3.0/mod-fuzzy.js, you instead have your module version and filename. Using emacs as Deno # emacs-ng offers the deno function in elisp, which allows users to leverage whatever Deno offers from the command line. For example, you can run Deno's repl (with elisp functions) by running the following: emacs --batch --eval '(deno \"repl\")' You can use Deno's formatter by running emacs --batch --eval '(deno \"fmt\")' You could even run a script via emacs --batch --eval '(deno \"run\" \"--allow-read\" \"test.ts\")' NOTE: You need to specify read/write/etc. permissions. Think of this as if you were using emacs AS Deno. The Deno function takes the exact same flags as the Deno application. It's designed for use in batch mode on the command line, however it can also be used in regular elisp. Where to go next # We don't want to duplicate Deno's excellent documentation, so it's recommended you read their manual for their standard library, and their examples.","title":"Using Deno"},{"location":"js/using-deno/#using-the-power-of-deno","text":"","title":"Using the power of Deno"},{"location":"js/using-deno/#what-is-deno","text":"Deno is a program that is similar to Node.js, except that it is written in Rust. Both Deno and Node.js were created by the same person: Ryan Dhal. While normally you would invoke Deno via the command line, the emacs-ng project has integrated the Deno runtime into the emacs-ng client directly. Deno is powered by v8, Chrome's Open Source JavaScript engine. A JavaScript engine is more limited than most users realize. For example, utilities like XMLHttpRequest are not provided directly by the JavaScript engine, but are instead provided by a runtime (like your browser). Deno provides interfaces for performing I/O operations like file reads/writes, network operations, and spawning subprocesses. By default, emacs-ng will allow reading, writing, network, and subprocess operations. In Deno's examples, you may see them advise you to pass flags like \"--allow-net\". These are not needed when executing emacs-ng code. If you want to globally disable any of the previously mentioned operations, you can run the following lisp prior to executing JavaScript: (js-initialize :allow-net nil :allow-read nil :allow-write nil :allow-run nil) You can define any combination of the above arguments. See js-initialize's documentation for more information.","title":"What is Deno?"},{"location":"js/using-deno/#general-documentation-standard-library","text":"Deno has excellent documentation . This guide is to give you a basic familiarity with Deno to help you to quickly write applications, but is FAR from all inclusive. Deno maintains a powerful standard library at https://deno.land/std@0.83.0 . Importing a Deno std module is simple: just include this in your JavaScript file/buffer: Credit to https://deno.land/std@0.83.0/fs for the example: import { copy , copySync } from \"https://deno.land/std@0.83.0/fs/mod.ts\" ; copy ( \"./foo\" , \"./bar\" ); // returns a promise copySync ( \"./foo\" , \"./bar\" ); // void copySync ( \"./foo\" , \"./existingFolder\" , { overwrite : true }); The first thing you may notice is that we are importing a URL, not a local filepath. Deno allows you to download dependencies from the network. This file will only be downloaded once and compiled once, and it's results will be cached on your local file system. After initial download, you will use your local filesystem's copy instead of using the network. By default, all Deno API's are asynchronous . Almost all async operations have an alternate version that is synchronous. That means that when you make a call to read a file, or walk a directory, it is returning a Promise. Deno has the naming convention that the synchronous versions all end in \" Sync\". A common point of confusion with emacs-ng is that when you evaluate a file, a buffer, or even an anonymous block in JavaScript via (eval-js) , you are executing your code within a JavaScript module with top level await enabled. What does that mean? If you invoke await within your top level module, you will block the main thread until completion. Looking again with our example above with that in mind: import { copy } from \"https://deno.land/std@0.83.0/fs/mod.ts\" ; // This block the main thread, including lisp execution, until this action is completed await copy ( \"./foo\" , \"./bar\" ); // This does not block JavaScript or Lisp, instead our .then will be executed once // the async action is complete. copy ( \"./foo\" , \"./bar\" ). then (() => console . log ( \"Complete\" )); Remember that async/await in JavaScript is just syntax sugar over Promises. There may be times where you want to use the top level await functionality to block on a promise at a certain time.","title":"General Documentation / Standard Library"},{"location":"js/using-deno/#distribution","text":"Once you have created your great emacs-ng module, how do you distribute it? Normally you would go through a repository like ELPA or MELPLA. While that is still a possibility, you have a third option, which is Deno's user modules . Navigating to the link below gives you the information on the upload process, but in the author's opinion, it is very simple and streamlined. Once your module is uploaded, you can have your user's include a line similar to this in their init.el (eval-js \"import 'https://deno.land/x/fuzzy_search@0.3.0/mod-fuzzy.js'\") Where instead of fuzzy_search@0.3.0/mod-fuzzy.js, you instead have your module version and filename.","title":"Distribution"},{"location":"js/using-deno/#using-emacs-as-deno","text":"emacs-ng offers the deno function in elisp, which allows users to leverage whatever Deno offers from the command line. For example, you can run Deno's repl (with elisp functions) by running the following: emacs --batch --eval '(deno \"repl\")' You can use Deno's formatter by running emacs --batch --eval '(deno \"fmt\")' You could even run a script via emacs --batch --eval '(deno \"run\" \"--allow-read\" \"test.ts\")' NOTE: You need to specify read/write/etc. permissions. Think of this as if you were using emacs AS Deno. The Deno function takes the exact same flags as the Deno application. It's designed for use in batch mode on the command line, however it can also be used in regular elisp.","title":"Using emacs as Deno"},{"location":"js/using-deno/#where-to-go-next","text":"We don't want to duplicate Deno's excellent documentation, so it's recommended you read their manual for their standard library, and their examples.","title":"Where to go next"}]}